{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentinel-1 TOPS stack processor\n",
    "The detailed algorithm for stack processing of TOPS data can be find here:\n",
    "\n",
    "+ Fattahi, H., P. Agram, and M. Simons (2016), A Network-Based Enhanced Spectral Diversity Approach for TOPS Time-Series Analysis, IEEE Transactions on Geoscience and Remote Sensing, 55(2), 777-786, doi:[10.1109/TGRS.2016.2614925](https://ieeexplore.ieee.org/abstract/document/7637021).\n",
    "\n",
    "-----------------------------------\n",
    "\n",
    "The scripts provides support for Sentinel-1 TOPS stack processing. Currently supported workflows include a coregistered stack of SLC, interferograms, offsets, and coherence. \n",
    "\n",
    "\n",
    "##To use the sentinel stack processor, make sure to add the path of your `contrib/stack/topsStack` folder to your `$PATH` environment varibale. \n",
    "\n",
    "#### Be sure [default] credentials in ~/.aws/credentials are valid\n",
    "\n",
    "#### Additional installs needed (will add later down the line)\n",
    "```\n",
    "conda install -c conda-forge parallel -y\n",
    "pip install awscli\n",
    "pip install opencv-python-headless\n",
    "pip install git+https://github.com/hysds/osaka.git#egg=osaka\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: osaka from git+https://github.com/hysds/osaka.git#egg=osaka in /opt/conda/lib/python3.7/site-packages (1.0.4)\n",
      "Requirement already satisfied: filechunkio==1.6.0 in /opt/conda/lib/python3.7/site-packages (from osaka) (1.6)\n",
      "Requirement already satisfied: google-cloud-storage>=0.22.0 in /opt/conda/lib/python3.7/site-packages (from osaka) (1.33.0)\n",
      "Requirement already satisfied: six>=1.10.0 in /opt/conda/lib/python3.7/site-packages (from osaka) (1.15.0)\n",
      "Requirement already satisfied: easywebdav==1.2.0 in /opt/conda/lib/python3.7/site-packages (from osaka) (1.2.0)\n",
      "Requirement already satisfied: boto3>=1.11.1 in /opt/conda/lib/python3.7/site-packages (from osaka) (1.16.21)\n",
      "Requirement already satisfied: configparser>=3.5.0 in /opt/conda/lib/python3.7/site-packages (from osaka) (5.0.1)\n",
      "Requirement already satisfied: requests>=2.7.0 in /opt/conda/lib/python3.7/site-packages (from osaka) (2.24.0)\n",
      "Requirement already satisfied: future>=0.17.1 in /opt/conda/lib/python3.7/site-packages (from osaka) (0.18.2)\n",
      "Requirement already satisfied: awscli>=1.17.1 in /opt/conda/lib/python3.7/site-packages (from osaka) (1.18.181)\n",
      "Requirement already satisfied: azure-storage-blob==1.4.0 in /opt/conda/lib/python3.7/site-packages (from osaka) (1.4.0)\n",
      "Requirement already satisfied: backoff>=1.3.1 in /opt/conda/lib/python3.7/site-packages (from osaka) (1.10.0)\n",
      "Requirement already satisfied: google-cloud-core<2.0dev,>=1.4.1 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage>=0.22.0->osaka) (1.4.3)\n",
      "Requirement already satisfied: google-resumable-media<2.0dev,>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage>=0.22.0->osaka) (1.1.0)\n",
      "Requirement already satisfied: google-auth<2.0dev,>=1.11.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage>=0.22.0->osaka) (1.23.0)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/conda/lib/python3.7/site-packages (from boto3>=1.11.1->osaka) (0.10.0)\n",
      "Requirement already satisfied: botocore<1.20.0,>=1.19.21 in /opt/conda/lib/python3.7/site-packages (from boto3>=1.11.1->osaka) (1.19.21)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /opt/conda/lib/python3.7/site-packages (from boto3>=1.11.1->osaka) (0.3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.7.0->osaka) (2020.11.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=2.7.0->osaka) (1.25.10)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.7.0->osaka) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests>=2.7.0->osaka) (3.0.4)\n",
      "Requirement already satisfied: colorama<0.4.4,>=0.2.5; python_version != \"3.4\" in /opt/conda/lib/python3.7/site-packages (from awscli>=1.17.1->osaka) (0.4.3)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /opt/conda/lib/python3.7/site-packages (from awscli>=1.17.1->osaka) (0.15.2)\n",
      "Requirement already satisfied: rsa<=4.5.0,>=3.1.2; python_version != \"3.4\" in /opt/conda/lib/python3.7/site-packages (from awscli>=1.17.1->osaka) (4.5)\n",
      "Requirement already satisfied: PyYAML<5.4,>=3.10; python_version != \"3.4\" in /opt/conda/lib/python3.7/site-packages (from awscli>=1.17.1->osaka) (5.3.1)\n",
      "Requirement already satisfied: azure-storage-common~=1.4 in /opt/conda/lib/python3.7/site-packages (from azure-storage-blob==1.4.0->osaka) (1.4.2)\n",
      "Requirement already satisfied: azure-common>=1.1.5 in /opt/conda/lib/python3.7/site-packages (from azure-storage-blob==1.4.0->osaka) (1.1.26)\n",
      "Requirement already satisfied: google-api-core<2.0.0dev,>=1.19.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-core<2.0dev,>=1.4.1->google-cloud-storage>=0.22.0->osaka) (1.23.0)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0; python_version >= \"3.5\" in /opt/conda/lib/python3.7/site-packages (from google-resumable-media<2.0dev,>=1.0.0->google-cloud-storage>=0.22.0->osaka) (1.0.0)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<2.0dev,>=1.11.0->google-cloud-storage>=0.22.0->osaka) (50.3.0.post20201006)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<2.0dev,>=1.11.0->google-cloud-storage>=0.22.0->osaka) (4.1.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<2.0dev,>=1.11.0->google-cloud-storage>=0.22.0->osaka) (0.2.8)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.7/site-packages (from botocore<1.20.0,>=1.19.21->boto3>=1.11.1->osaka) (2.8.1)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /opt/conda/lib/python3.7/site-packages (from rsa<=4.5.0,>=3.1.2; python_version != \"3.4\"->awscli>=1.17.1->osaka) (0.4.8)\n",
      "Requirement already satisfied: cryptography in /opt/conda/lib/python3.7/site-packages (from azure-storage-common~=1.4->azure-storage-blob==1.4.0->osaka) (3.1.1)\n",
      "Requirement already satisfied: pytz in /opt/conda/lib/python3.7/site-packages (from google-api-core<2.0.0dev,>=1.19.0->google-cloud-core<2.0dev,>=1.4.1->google-cloud-storage>=0.22.0->osaka) (2020.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2.0.0dev,>=1.19.0->google-cloud-core<2.0dev,>=1.4.1->google-cloud-storage>=0.22.0->osaka) (1.52.0)\n",
      "Requirement already satisfied: protobuf>=3.12.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2.0.0dev,>=1.19.0->google-cloud-core<2.0dev,>=1.4.1->google-cloud-storage>=0.22.0->osaka) (3.14.0)\n",
      "Requirement already satisfied: cffi>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from google-crc32c<2.0dev,>=1.0; python_version >= \"3.5\"->google-resumable-media<2.0dev,>=1.0.0->google-cloud-storage>=0.22.0->osaka) (1.14.0)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.7/site-packages (from cffi>=1.0.0->google-crc32c<2.0dev,>=1.0; python_version >= \"3.5\"->google-resumable-media<2.0dev,>=1.0.0->google-cloud-storage>=0.22.0->osaka) (2.20)\n",
      "Requirement already satisfied: opencv-python-headless in /opt/conda/lib/python3.7/site-packages (4.4.0.46)\n",
      "Requirement already satisfied: numpy>=1.14.5 in /opt/conda/lib/python3.7/site-packages (from opencv-python-headless) (1.18.1)\n",
      "Collecting package metadata (current_repodata.json): ...working... done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "Collecting package metadata (current_repodata.json): ...working... done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'topsstack-hamsar' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd\n",
    "git clone --single-branch -b develop-j2-50percent-NSDS-1099 https://github.com/aria-jpl/topsstack-hamsar.git\n",
    "pip install git+https://github.com/hysds/osaka.git#egg=osaka\n",
    "pip install opencv-python-headless\n",
    "conda install -c conda-forge fiona -y\n",
    "conda install -c conda-forge parallel -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "    source /opt/conda/etc/profile.d/conda.sh\n",
    "    source /opt/isce2/isce_env.sh\n",
    "    export PYTHONPATH=/opt/isce2/src/isce2/contrib/stack/topsStack/:$PYTHONPATH\n",
    "    export PATH=/opt/isce2/src/isce2/contrib/stack/topsStack/:$PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from math import floor, ceil\n",
    "import json\n",
    "import re\n",
    "import osaka\n",
    "import osaka.main\n",
    "from builtins import str\n",
    "import os, sys, re, json, logging, traceback, requests, argparse\n",
    "from datetime import datetime\n",
    "from pprint import pformat\n",
    "from requests.packages.urllib3.exceptions import (InsecureRequestWarning,\n",
    "                                                  InsecurePlatformWarning)\n",
    "try: from html.parser import HTMLParser\n",
    "except: from html.parser import HTMLParser\n",
    "        \n",
    "requests.packages.urllib3.disable_warnings(InsecureRequestWarning)\n",
    "requests.packages.urllib3.disable_warnings(InsecurePlatformWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "SLC_RE = re.compile(r'(?P<mission>S1\\w)_IW_SLC__.*?' +\n",
    "                    r'_(?P<start_year>\\d{4})(?P<start_month>\\d{2})(?P<start_day>\\d{2})' +\n",
    "                    r'T(?P<start_hour>\\d{2})(?P<start_min>\\d{2})(?P<start_sec>\\d{2})' +\n",
    "                    r'_(?P<end_year>\\d{4})(?P<end_month>\\d{2})(?P<end_day>\\d{2})' +\n",
    "                    r'T(?P<end_hour>\\d{2})(?P<end_min>\\d{2})(?P<end_sec>\\d{2})_.*$')\n",
    "ISCE_HOME=\"/opt/isce2/isce\"\n",
    "\n",
    "QC_SERVER = 'https://qc.sentinel1.eo.esa.int/'\n",
    "DATA_SERVER = 'http://aux.sentinel1.eo.esa.int/'\n",
    "\n",
    "ORBITMAP = [('precise','aux_poeorb', 100),\n",
    "            ('restituted','aux_resorb', 100)]\n",
    "\n",
    "OPER_RE = re.compile(r'S1\\w_OPER_AUX_(?P<type>\\w+)_OPOD_(?P<yr>\\d{4})(?P<mo>\\d{2})(?P<dy>\\d{2})')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "global runtime_dict\n",
    "runtime_dict = {}\n",
    "ISCE_HOME=\"/opt/isce2/isce\"\n",
    "ctx = {}\n",
    "ctx[\"min_lat\"] = 34.6002832\n",
    "ctx[\"max_lat\"] = 34.6502392\n",
    "ctx[\"min_lon\"] = -79.0801608\n",
    "ctx[\"max_lon\"] = -78.9705888\n",
    "ctx[\"master_date\"]=\"\"\n",
    "ctx[\"localize_slcs\"]= [\"S1A_IW_SLC__1SDV_20150315T231319_20150315T231349_005049_006569_0664\",\n",
    "                      \"S1A_IW_SLC__1SDV_20150818T231326_20150818T231356_007324_00A0E0_93D5\",\n",
    "                      \"S1A_IW_SLC__1SDV_20150830T231332_20150830T231402_007499_00A5A9_02B3\",\n",
    "                      \"S1A_IW_SLC__1SDV_20160414T231321_20160414T231350_010824_01030B_F02B\",\n",
    "                      \"S1A_IW_SLC__1SDV_20160414T231348_20160414T231416_010824_01030B_9FA9\"\n",
    "                     ]\n",
    "wd = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('_stdout.txt', 'w') as f:\n",
    "    f.write(\"Output File\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyHTMLParser(HTMLParser):\n",
    "\n",
    "    def __init__(self):\n",
    "        HTMLParser.__init__(self)\n",
    "        self.fileList = []\n",
    "        self.pages = 0\n",
    "        self.in_td = False\n",
    "        self.in_a = False\n",
    "        self.in_ul = False\n",
    "\n",
    "    def handle_starttag(self, tag, attrs):\n",
    "        if tag == 'td':\n",
    "            self.in_td = True\n",
    "        elif tag == 'a' and self.in_td:\n",
    "            self.in_a = True\n",
    "        elif tag == 'ul':\n",
    "            for k,v in attrs:\n",
    "                if k == 'class' and v.startswith('pagination'):\n",
    "                    self.in_ul = True\n",
    "        elif tag == 'li' and self.in_ul:\n",
    "            self.pages += 1\n",
    "\n",
    "    def handle_data(self,data):\n",
    "        if self.in_td and self.in_a:\n",
    "            if OPER_RE.search(data):\n",
    "                self.fileList.append(data.strip())\n",
    "\n",
    "    def handle_endtag(self, tag):\n",
    "        if tag == 'td':\n",
    "            self.in_td = False\n",
    "            self.in_a = False\n",
    "        elif tag == 'a' and self.in_td:\n",
    "            self.in_a = False\n",
    "        elif tag == 'ul' and self.in_ul:\n",
    "            self.in_ul = False\n",
    "        elif tag == 'html':\n",
    "            if self.pages == 0:\n",
    "                self.pages = 1\n",
    "            else:\n",
    "                # decrement page back and page forward list items\n",
    "                self.pages -= 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def session_get(session, url):\n",
    "    return session.get(url, verify=False)\n",
    "\n",
    "def get_download_orbit_dict(download_orbit_dict, slc_date, mission_type):\n",
    "    \n",
    "\n",
    "    url = \"https://qc.sentinel1.eo.esa.int/aux_poeorb/?validity_start={}&sentinel1__mission={}\".format(slc_date, mission_type)\n",
    "    session = requests.Session()\n",
    "    r = session_get(session, url)\n",
    "    r.raise_for_status()\n",
    "    parser = MyHTMLParser()\n",
    "    parser.feed(r.text)\n",
    "\n",
    "    for res in parser.fileList:\n",
    "        #id = \"%s-%s\" % (os.path.splitext(res)[0], dataset_version)\n",
    "        match = OPER_RE.search(res)\n",
    "        if not match:\n",
    "            raise RuntimeError(\"Failed to parse orbit: {}\".format(res))\n",
    "        download_orbit_dict[res] = os.path.join(DATA_SERVER, \"/\".join(match.groups()), \"{}.EOF\".format(res))\n",
    "        #yield id, results[id]\n",
    "        \n",
    "    #print(results)\n",
    "    \n",
    "    return download_orbit_dict\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_orbit_files():\n",
    "    from datetime import datetime\n",
    "    import json\n",
    "    import os\n",
    "    import osaka\n",
    "    import osaka.main\n",
    "    \n",
    "    orbit_dict = {}\n",
    "    \n",
    "    \n",
    "    \n",
    "    orbit_dates = []\n",
    "    \n",
    "    for slc in ctx[\"localize_slcs\"]:\n",
    "        match = SLC_RE.search(slc)\n",
    "        if not match:\n",
    "            raise RuntimeError(\"Failed to recognize SLC ID %s.\" %slc)\n",
    "        mission = match.group('mission')\n",
    "        day_dt_str = \"{}-{}-{}\".format(match.group('start_year'), \n",
    "                                       match.group('start_month'), match.group('start_day'))\n",
    "        \n",
    "        if day_dt_str not in orbit_dates:\n",
    "            orbit_dates.append(day_dt_str)\n",
    "            orbit_dict = get_download_orbit_dict(orbit_dict, day_dt_str, mission)\n",
    "            directory = os.path.join(wd, \"orbits\")\n",
    "            for k, v in orbit_dict.items():\n",
    "                osaka.main.get(v, directory)\n",
    "            \n",
    "            \n",
    "                \n",
    "    print(\"orbit_dict : %s \" %json.dumps(orbit_dict, indent=4))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_current_time():\n",
    "    import datetime\n",
    "    return datetime.datetime.now()\n",
    "\n",
    "def download_slc(slc_id, path):\n",
    "    \n",
    "    \n",
    "    url = \"https://datapool.asf.alaska.edu/SLC/SA/{}.zip\".format(slc_id)\n",
    "    print(\"Downloading {} : {}\".format(slc_id, url))\n",
    "    \n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    osaka.main.get(url, path)\n",
    "\n",
    "def run_cmd_output(cmd):\n",
    "    from subprocess import check_output, CalledProcessError\n",
    "    cmd_line = \" \".join(cmd)\n",
    "    print(\"Calling: {}\".format(cmd_line))\n",
    "    output = check_output(cmd_line, shell=True)\n",
    "    return output\n",
    "\n",
    "def run_cmd(cmd):\n",
    "    !source /opt/isce2/isce_env.sh\n",
    "    !export PATH=/opt/conda/bin/:/opt/isce2/src/isce2/contrib/stack/topsStack/:${PATH}\n",
    "    !export PYTHONPATH=/opt/isce2/src/isce2/contrib/stack/topsStack/:${PYTHONPATH}\n",
    "    import subprocess\n",
    "    from subprocess import check_call, CalledProcessError\n",
    "    import sys\n",
    "    cmd_line = \" \".join(cmd)\n",
    "    print(\"Calling : {}\".format(cmd_line))\n",
    "    p = subprocess.Popen(cmd_line, shell=True,stdout=subprocess.PIPE,stderr=subprocess.PIPE)\n",
    "    while True: \n",
    "        line = p.stdout.readline()\n",
    "        if not line:\n",
    "            break\n",
    "        print(line.strip())\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "def get_minimum_bounding_rectangle():\n",
    "    from math import floor, ceil\n",
    "    cwd = os.getcwd()\n",
    "    slc_ids = [x for x in os.listdir('.') if os.path.isdir(x) and '_SLC__' in x]\n",
    "\n",
    "    all_lats = []\n",
    "    all_lons = []\n",
    "    for slc in slc_ids:\n",
    "        slc_met_json = slc + '.met.json'\n",
    "\n",
    "        with open(os.path.join(cwd, slc, slc_met_json), 'r') as f:\n",
    "            data = json.load(f)\n",
    "            bbox = data['bbox']\n",
    "            for coord in bbox:\n",
    "                all_lats.append(coord[0])\n",
    "                all_lons.append(coord[1])\n",
    "\n",
    "    min_lat = min(all_lats) + 0.2\n",
    "    max_lat = max(all_lats) - 0.1\n",
    "    min_lon = min(all_lons)\n",
    "    max_lon = max(all_lons)\n",
    "\n",
    "    min_lat_lo = floor(min_lat)\n",
    "    max_lat_hi = ceil(max_lat)\n",
    "    min_lon_lo = floor(min_lon)\n",
    "    max_lon_hi = ceil(max_lon)\n",
    "\n",
    "    return min_lat, max_lat, min_lon, max_lon, min_lat_lo, max_lat_hi, min_lon_lo, max_lon_hi\n",
    "\n",
    "def get_user_input_bbox(ctx_file):\n",
    "    \"\"\"\n",
    "    :param ctx_file: dictionary from cxt file\n",
    "    :return: void\n",
    "    \"\"\"\n",
    "    from math import floor, ceil\n",
    "    min_lat = ctx_file['min_lat']\n",
    "    max_lat = ctx_file['max_lat']\n",
    "    min_lon = ctx_file['min_lon']\n",
    "    max_lon = ctx_file['max_lon']\n",
    "\n",
    "    min_lat_lo = floor(min_lat)\n",
    "    max_lat_hi = ceil(max_lat)\n",
    "    min_lon_lo = floor(min_lon)\n",
    "    max_lon_hi = ceil(max_lon)\n",
    "\n",
    "    return min_lat, max_lat, min_lon, max_lon, min_lat_lo, max_lat_hi, min_lon_lo, max_lon_hi\n",
    "\n",
    "def get_master_date(ctx):\n",
    "    master_date = ctx.get('master_date', \"\")\n",
    "    return master_date\n",
    "\n",
    "def get_bbox(ctx):\n",
    "    # min_lat, max_lat, min_lon, max_lon = ctx['region_of_interest']\n",
    "\n",
    "    if ctx['min_lat'] != \"\" or ctx['max_lat'] != \"\" or ctx['min_lon'] != \"\" or ctx['max_lon'] != \"\":\n",
    "        # if any values are present in _context.json we can assume user put them in manually\n",
    "        bbox_data = get_user_input_bbox(ctx)\n",
    "    else:\n",
    "        # if user did not define ANY lat lons\n",
    "        bbox_data = get_minimum_bounding_rectangle()\n",
    "\n",
    "    return bbox_data\n",
    "\n",
    "def download_dem():\n",
    "    dem_cmd = [\n",
    "        \"{}/applications/dem.py\".format(ISCE_HOME), \"-a\",\n",
    "        \"stitch\", \"-b\", \"{} {} {} {}\".format(MINLAT_LO, MAXLAT_HI, MINLON_LO, MAXLON_HI),\n",
    "        \"-r\", \"-s\", \"1\", \"-f\", \"-c\", \"|\", \"tee\", \"dem.txt\"\n",
    "        #\"-n\", dem_user, \"-w\", dem_pass,\"-u\", dem_url\n",
    "    ]\n",
    "    run_cmd(dem_cmd)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import json\n",
    "import re\n",
    "\n",
    "from datetime import datetime\n",
    "from osgeo import ogr, osr\n",
    "\n",
    "\n",
    "# copied from stitch_ifgs.get_union_polygon()\n",
    "def get_union_polygon(ds_files):\n",
    "    \"\"\"\n",
    "    Get GeoJSON polygon of union of IFGs.\n",
    "    :param ds_files: list of .dataset.json files, which have the 'location' key\n",
    "    :return: geojson of merged bbox\n",
    "    \"\"\"\n",
    "\n",
    "    geom_union = None\n",
    "    for ds_file in ds_files:\n",
    "        f = open(ds_file)\n",
    "        ds = json.load(f)\n",
    "        geom = ogr.CreateGeometryFromJson(json.dumps(ds['location'], indent=2, sort_keys=True))\n",
    "        if geom_union is None:\n",
    "            geom_union = geom\n",
    "        else:\n",
    "            geom_union = geom_union.Union(geom)\n",
    "    return json.loads(geom_union.ExportToJson()), geom_union.GetEnvelope()\n",
    "\n",
    "\n",
    "def get_dataset_met_json_files(cxt):\n",
    "    \"\"\"\n",
    "    returns 2 lists: file paths for dataset.json files and met.json files\n",
    "    :param cxt: json from _context.json\n",
    "    :return: list[str], list[str]\n",
    "    \"\"\"\n",
    "    pwd = os.getcwd()\n",
    "    localize_urls = cxt['localize_urls']\n",
    "\n",
    "    met_files, ds_files = [], []\n",
    "    for localize_url in localize_urls:\n",
    "        local_path = localize_url['local_path']\n",
    "        slc_id = local_path.split('/')[0]\n",
    "        slc_path = os.path.join(pwd, slc_id, slc_id)\n",
    "\n",
    "        ds_files.append(slc_path + '.dataset.json')\n",
    "        met_files.append(slc_path + '.met.json')\n",
    "    return ds_files, met_files\n",
    "\n",
    "\n",
    "def get_scenes(cxt):\n",
    "    \"\"\"\n",
    "    gets all SLC scenes for the stack\n",
    "    :param cxt: contents for _context.json\n",
    "    :return: list of scenes\n",
    "    \"\"\"\n",
    "    localize_urls = cxt['localize_urls']\n",
    "    all_scenes = set()\n",
    "    for localize_url in localize_urls:\n",
    "        local_path = localize_url['local_path']\n",
    "        slc_id = local_path.split('/')[0]\n",
    "        all_scenes.add(slc_id)\n",
    "    return sorted(list(all_scenes))\n",
    "\n",
    "\n",
    "def get_min_max_timestamps(scenes_ls):\n",
    "    \"\"\"\n",
    "    returns the min timestamp and max timestamp of the stack\n",
    "    :param scenes_ls: list[str] all slc scenes in stack\n",
    "    :return: (str, str) 2 timestamp strings, ex. 20190518T161611\n",
    "    \"\"\"\n",
    "    timestamps = set()\n",
    "\n",
    "    regex_pattern = r'(\\d{8}T\\d{6}).(\\d{8}T\\d{6})'\n",
    "    for scene in scenes_ls:\n",
    "        matches = re.search(regex_pattern, scene)\n",
    "        if not matches:\n",
    "            raise Exception(\"regex %s was unable to match with SLC id %s\" % (regex_pattern, scene))\n",
    "\n",
    "        slc_timestamps = (matches.group(1), matches.group(2))\n",
    "        timestamps = timestamps.union(slc_timestamps)\n",
    "\n",
    "    min_timestamp = min(timestamps)\n",
    "    max_timestamp = max(timestamps)\n",
    "    return min_timestamp.replace('T', ''), max_timestamp.replace('T', '')\n",
    "\n",
    "\n",
    "def create_list_from_keys_json_file(json_files, *args):\n",
    "    \"\"\"\n",
    "    gets all key values in each .json file and returns a sorted array of values\n",
    "    :param json_files: list[str]\n",
    "    :return: list[]\n",
    "    \"\"\"\n",
    "    values = set()\n",
    "    for json_file in json_files:\n",
    "        if os.path.isfile(json_file): \n",
    "            f = open(json_file)\n",
    "            data = json.load(f)\n",
    "            for arg in args:\n",
    "                value = data[arg]\n",
    "                values.add(value)\n",
    "    return sorted(list(values))\n",
    "\n",
    "\n",
    "def camelcase_to_underscore(name):\n",
    "    s1 = re.sub('(.)([A-Z][a-z]+)', r'\\1_\\2', name)\n",
    "    return re.sub('([a-z0-9])([A-Z])', r'\\1_\\2', s1).lower()\n",
    "\n",
    "\n",
    "def get_key_and_convert_to_underscore(json_file_paths, key):\n",
    "    \"\"\"\n",
    "    read through all the json files in file paths, get the first occurrence of key and convert it to underscore\n",
    "    :param json_file_paths: list[str]\n",
    "    :param key: str\n",
    "    :return: key and value\n",
    "    \"\"\"\n",
    "    for json_file in json_file_paths:\n",
    "        if os.path.isfile(json_file): \n",
    "            f = open(json_file)\n",
    "            data = json.load(f)\n",
    "            if key in data.keys():\n",
    "                underscore_key = camelcase_to_underscore(key)\n",
    "                return underscore_key, data[key]\n",
    "    return None, None\n",
    "\n",
    "\n",
    "def generate_dataset_json_data(dataset_json_files, version):\n",
    "    \"\"\"\n",
    "    :param cxt: _context.json file\n",
    "    :param dataset_json_files: list[str] all file paths of SLC's .dataset.json files\n",
    "    :param version: str: version, ex. v1.0\n",
    "    :return: dict\n",
    "    \"\"\"\n",
    "    dataset_json_data = dict()\n",
    "    dataset_json_data['version'] = version\n",
    "\n",
    "    sensing_timestamps = create_list_from_keys_json_file(dataset_json_files, 'starttime', 'endtime')\n",
    "    dataset_json_data['starttime'] = min(sensing_timestamps)\n",
    "    dataset_json_data['endtime'] = max(sensing_timestamps)\n",
    "\n",
    "    geojson, image_corners = get_union_polygon(dataset_json_files)\n",
    "    dataset_json_data['location'] = geojson\n",
    "\n",
    "    return dataset_json_data\n",
    "\n",
    "\n",
    "def generate_met_json_data(cxt, met_json_file_paths, dataset_json_files, version):\n",
    "    \"\"\"\n",
    "    :param cxt: _context.json file\n",
    "    :param met_json_file_paths: list[str] all file paths of SLC's .met.json files\n",
    "    :param dataset_json_files: list[str] all file paths of SLC's .dataset.json files\n",
    "    :param version: str: version, ex. v1.0\n",
    "    :return: dict\n",
    "    \"\"\"\n",
    "    met_json_data = {\n",
    "        'processing_start': os.environ['PROCESSING_START'],\n",
    "        'processing_stop': datetime.now().strftime('%Y-%m-%dT%H:%M:%S'),\n",
    "        'version': version\n",
    "    }\n",
    "\n",
    "    first_occurrence_keys = [\n",
    "        'direction',\n",
    "        'orbitNumber',\n",
    "        'trackNumber',\n",
    "        'sensor',\n",
    "        'platform'\n",
    "    ]\n",
    "    for key in first_occurrence_keys:\n",
    "        key, value = get_key_and_convert_to_underscore(met_json_file_paths, key)\n",
    "        met_json_data[key] = value\n",
    "\n",
    "    orbit_cycles = create_list_from_keys_json_file(met_json_file_paths, 'orbitCycle')\n",
    "    met_json_data['orbit_cycles'] = orbit_cycles\n",
    "\n",
    "    # generating bbox\n",
    "    geojson, image_corners = get_union_polygon(dataset_json_files)\n",
    "    coordinates = geojson['coordinates'][0]\n",
    "    for coordinate in coordinates:\n",
    "        coordinate[0], coordinate[1] = coordinate[1], coordinate[0]\n",
    "    met_json_data['bbox'] = coordinates\n",
    "\n",
    "    # list of SLC scenes\n",
    "    scenes = get_scenes(cxt)\n",
    "    met_json_data['scenes'] = scenes\n",
    "    met_json_data['scene_count'] = len(scenes)\n",
    "\n",
    "    # getting timestamps\n",
    "    sensing_timestamps = create_list_from_keys_json_file(dataset_json_files, 'starttime', 'endtime')\n",
    "    met_json_data['sensing_start'] = min(sensing_timestamps)\n",
    "    met_json_data['sensing_stop'] = max(sensing_timestamps)\n",
    "    met_json_data['timesteps'] = sensing_timestamps\n",
    "\n",
    "    # additional information\n",
    "    met_json_data['dataset_type'] = 'stack'\n",
    "\n",
    "    return met_json_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_context():\n",
    "    with open('_context.json', 'r') as f:\n",
    "        cxt = json.load(f)\n",
    "        return cxt\n",
    "\n",
    "\n",
    "def create_dataset():\n",
    "    VERSION = 'v1.0'\n",
    "    DATASET_NAMING_TEMPLATE = 'coregistered_slcs-{min_timestamp}-{max_timestamp}'\n",
    "    PWD = os.getcwd()\n",
    "\n",
    "    # creating list of all SLC .dataset.json and .met.json files\n",
    "    context_json = read_context()\n",
    "    dataset_json_files, met_json_files = get_dataset_met_json_files(context_json)\n",
    "\n",
    "    # getting list of SLC scenes and extracting min max timestamp\n",
    "    slc_scenes = get_scenes(context_json)\n",
    "    min_timestamp, max_timestamp = get_min_max_timestamps(slc_scenes)\n",
    "\n",
    "    # creatin dataset directory\n",
    "    dataset_name = DATASET_NAMING_TEMPLATE.format(min_timestamp=min_timestamp, max_timestamp=max_timestamp)\n",
    "    if not os.path.exists(dataset_name):\n",
    "        os.mkdir(dataset_name)\n",
    "\n",
    "    # move merged/ master/ slaves/ directory to dataset directory\n",
    "    move_directories = ['merged', 'master', 'slaves']\n",
    "    for directory in move_directories:\n",
    "        shutil.move(directory, dataset_name)\n",
    "\n",
    "    # move _stdout.txt log file to dataset\n",
    "    shutil.copyfile('_stdout.txt', os.path.join(dataset_name, '_stdout.txt'))\n",
    "\n",
    "    # generate .dataset.json data\n",
    "    dataset_json_data = generate_dataset_json_data(dataset_json_files, VERSION)\n",
    "    dataset_json_data['label'] = dataset_name\n",
    "    print(json.dumps(dataset_json_data, indent=2))\n",
    "\n",
    "    # generate .met.json data\n",
    "    met_json_data = generate_met_json_data(context_json, met_json_files, dataset_json_files, VERSION)\n",
    "    print(json.dumps(met_json_data, indent=2))\n",
    "\n",
    "    # writing .dataset.json to file\n",
    "    dataset_json_filename = os.path.join(PWD, dataset_name, dataset_name + '.dataset.json')\n",
    "    with open(dataset_json_filename, 'w') as f:\n",
    "        json.dump(dataset_json_data, f, indent=2)\n",
    "\n",
    "    # writing .met.json to file\n",
    "    met_json_filename = os.path.join(PWD, dataset_name, dataset_name + '.met.json')\n",
    "    with open(met_json_filename, 'w') as f:\n",
    "        json.dump(met_json_data, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34.6002832 34.6502392 -79.0801608 -78.9705888 34 35 -80 -78\n"
     ]
    }
   ],
   "source": [
    "MINLAT, MAXLAT, MINLON, MAXLON, MINLAT_LO, MAXLAT_HI, MINLON_LO, MAXLON_HI =get_bbox(ctx)\n",
    "print(\"{} {} {} {} {} {} {} {}\".format(MINLAT, MAXLAT, MINLON, MAXLON, MINLAT_LO, MAXLAT_HI, MINLON_LO, MAXLON_HI))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Download Sentinel-1 data SLC ####\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_slcs(path):\n",
    "    \n",
    "    for slc in ctx[\"localize_slcs\"]:\n",
    "        download_slc(slc, path)\n",
    "        \n",
    "path = \"zip\"\n",
    "download_slcs(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Download Orbit Files based onn SLC ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orbit_dict : {\n",
      "    \"S1A_OPER_AUX_POEORB_OPOD_20150406T122947_V20150315T225944_20150317T005944\": \"http://aux.sentinel1.eo.esa.int/POEORB/2015/04/06/S1A_OPER_AUX_POEORB_OPOD_20150406T122947_V20150315T225944_20150317T005944.EOF\",\n",
      "    \"S1A_OPER_AUX_POEORB_OPOD_20150908T122519_V20150818T225943_20150820T005943\": \"http://aux.sentinel1.eo.esa.int/POEORB/2015/09/08/S1A_OPER_AUX_POEORB_OPOD_20150908T122519_V20150818T225943_20150820T005943.EOF\",\n",
      "    \"S1A_OPER_AUX_POEORB_OPOD_20150920T122333_V20150830T225943_20150901T005943\": \"http://aux.sentinel1.eo.esa.int/POEORB/2015/09/20/S1A_OPER_AUX_POEORB_OPOD_20150920T122333_V20150830T225943_20150901T005943.EOF\",\n",
      "    \"S1A_OPER_AUX_POEORB_OPOD_20160505T121437_V20160414T225943_20160416T005943\": \"http://aux.sentinel1.eo.esa.int/POEORB/2016/05/05/S1A_OPER_AUX_POEORB_OPOD_20160505T121437_V20160414T225943_20160416T005943.EOF\"\n",
      "} \n"
     ]
    }
   ],
   "source": [
    "get_orbit_files()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Create your project folder somewhere ####\n",
    "\n",
    "```\n",
    "mkdir MexicoSenAT72\n",
    "cd MexicoSenAT72\n",
    "```\n",
    "\n",
    "#### 2. Prepare DEM ####\n",
    "\n",
    "Download of DEM (need to use wgs84 version) using the ISCE DEM download script.\n",
    "\n",
    "```\n",
    "mkdir DEM; cd DEM\n",
    "dem.py -a stitch -b 18 20 -100 -97 -r -s 1 –c\n",
    "rm demLat*.dem demLat*.dem.xml demLat*.dem.vrt\n",
    "cd ..\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling : /opt/isce2/isce/applications/dem.py -a stitch -b 34 35 -80 -78 -r -s 1 -f -c | tee dem.txt\n",
      "b'API open (R): ./demLat_N34_N35_Lon_W080_W078.dem'\n",
      "b'API close:  ./demLat_N34_N35_Lon_W080_W078.dem'\n",
      "b'GDAL open (R): ./demLat_N34_N35_Lon_W080_W078.dem.vrt'\n",
      "b'API open (WR): demLat_N34_N35_Lon_W080_W078.dem.wgs84'\n",
      "b''\n",
      "b'<< Geoid Correction I2 SRTM>>'\n",
      "b''\n",
      "b'Jet Propulsion Laboratory - Radar Science and Engineering'\n",
      "b''\n",
      "b''\n",
      "b'Sampling Geoid at grid points -  Longitude Samples:    23 Latitude Lines:    13'\n",
      "b'Corner Geoid Heights (m) =  -39.10 -33.37 -31.85 -37.08'\n",
      "b''\n",
      "b'Correcting data to geoid height...'\n",
      "b''\n",
      "b'At line:      512'\n",
      "b'At line:     1024'\n",
      "b'At line:     1536'\n",
      "b'At line:     2048'\n",
      "b'At line:     2560'\n",
      "b'At line:     3072'\n",
      "b'At line:     3584'\n",
      "b'GDAL close: ./demLat_N34_N35_Lon_W080_W078.dem.vrt'\n",
      "b'API close:  demLat_N34_N35_Lon_W080_W078.dem.wgs84'\n",
      "b'API open (R): demLat_N34_N35_Lon_W080_W078.dem.wgs84'\n",
      "b'API close:  demLat_N34_N35_Lon_W080_W078.dem.wgs84'\n",
      "b'Using default ISCE Path: /opt/isce2/isce'\n",
      "b'curl -n  -L -c $HOME/.earthdatacookie -b $HOME/.earthdatacookie -k -f -O http://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N34W080.SRTMGL1.hgt.zip'\n",
      "b'curl -n  -L -c $HOME/.earthdatacookie -b $HOME/.earthdatacookie -k -f -O http://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N34W079.SRTMGL1.hgt.zip'\n",
      "b'Writing geotrans to VRT for ./demLat_N34_N35_Lon_W080_W078.dem'\n",
      "b'Writing geotrans to VRT for demLat_N34_N35_Lon_W080_W078.dem.wgs84'\n",
      "b'Writing geotrans to VRT for /home/jovyan/nisar-on-demand-use-cases/demLat_N34_N35_Lon_W080_W078.dem.wgs84'\n",
      "b'N34W080.SRTMGL1.hgt.zip = succeded'\n",
      "b'N34W079.SRTMGL1.hgt.zip = succeded'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "download_dem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cwd : /home/jovyan/nisar-on-demand-use-cases\n",
      "Calling: awk '/wgs84/ {print $NF;exit}' dem.txt\n",
      "WGS84 : a/home/jovyan/nisar-on-demand-use-cases/demLat_N34_N35_Lon_W080_W078.dem.wgs84b\n",
      "Found wgs84 file: /home/jovyan/nisar-on-demand-use-cases/demLat_N34_N35_Lon_W080_W078.dem.wgs84\n",
      "Calling : /opt/isce2/isce/applications/fixImageXml.py --full -i /home/jovyan/nisar-on-demand-use-cases/demLat_N34_N35_Lon_W080_W078.dem.wgs84\n",
      "b'Using default ISCE Path: /opt/isce2/isce'\n",
      "b'Writing geotrans to VRT for /home/jovyan/nisar-on-demand-use-cases/demLat_N34_N35_Lon_W080_W078.dem.wgs84'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "cwd = os.getcwd()\n",
    "print(\"cwd : {}\".format(cwd))\n",
    "if os.path.exists(\"dem.txt\"):\n",
    "    cmd = [\"awk\", \"'/wgs84/ {print $NF;exit}'\", \"dem.txt\"]\n",
    "    WGS84 = run_cmd_output(cmd).decode(\"utf-8\").strip()\n",
    "    wgs84_file = os.path.join(cwd, WGS84)\n",
    "    print(\"WGS84 : a{}b\".format(wgs84_file))\n",
    "    if os.path.exists(wgs84_file):\n",
    "        print(\"Found wgs84 file: {}\".format(wgs84_file))\n",
    "        fix_cmd = [\"{}/applications/fixImageXml.py\".format(ISCE_HOME), \"--full\", \"-i\", \"{}\".format(wgs84_file) ]\n",
    "        run_cmd(fix_cmd) \n",
    "    else:\n",
    "        print(\"NO WGS84 FILE FOUND : {}\".format(wgs84_file))\n",
    "    \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AUX_CAL file download ####\n",
    "\n",
    "The following calibration auxliary (AUX_CAL) file is used for **antenna pattern correction** to compensate the range phase offset of SAFE products with **IPF verison 002.36** (mainly for images acquired before March 2015). If all your SAFE products are from another IPF version, then no AUX files are needed. Check [ESA document](https://earth.esa.int/documents/247904/1653440/Sentinel-1-IPF_EAP_Phase_correction) for details. \n",
    "\n",
    "Run the command below to download the AUX_CAL file once and store it somewhere (_i.e._ ~/aux/aux_cal) so that you can use it all the time, for `stackSentinel.py -a` or `auxiliary data directory` in `topsApp.py`.\n",
    "\n",
    "```\n",
    "wget https://qc.sentinel1.eo.esa.int/product/S1A/AUX_CAL/20140908T000000/S1A_AUX_CAL_V20140908T000000_G20190626T100201.SAFE.TGZ\n",
    "tar zxvf S1A_AUX_CAL_V20140908T000000_G20190626T100201.SAFE.TGZ\n",
    "rm S1A_AUX_CAL_V20140908T000000_G20190626T100201.SAFE.TGZ\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S1A_AUX_CAL_V20140908T000000_G20190626T100201.SAFE/\n",
      "S1A_AUX_CAL_V20140908T000000_G20190626T100201.SAFE/manifest.safe\n",
      "S1A_AUX_CAL_V20140908T000000_G20190626T100201.SAFE/data/\n",
      "S1A_AUX_CAL_V20140908T000000_G20190626T100201.SAFE/data/s1a-aux-cal.xml\n",
      "S1A_AUX_CAL_V20140908T000000_G20190626T100201.SAFE/support/\n",
      "S1A_AUX_CAL_V20140908T000000_G20190626T100201.SAFE/support/s1-aux-cal.xsd\n",
      "S1A_AUX_CAL_V20140908T000000_G20190626T100201.SAFE/support/s1-object-types.xsd\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--2020-11-19 18:33:32--  https://qc.sentinel1.eo.esa.int/product/S1A/AUX_CAL/20140908T000000/S1A_AUX_CAL_V20140908T000000_G20190626T100201.SAFE.TGZ\n",
      "Resolving qc.sentinel1.eo.esa.int (qc.sentinel1.eo.esa.int)... 131.176.235.71\n",
      "Connecting to qc.sentinel1.eo.esa.int (qc.sentinel1.eo.esa.int)|131.176.235.71|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 493780 (482K) [application/x-gzip]\n",
      "Saving to: ‘S1A_AUX_CAL_V20140908T000000_G20190626T100201.SAFE.TGZ’\n",
      "\n",
      "     0K .......... .......... .......... .......... .......... 10% 86.2K 5s\n",
      "    50K .......... .......... .......... .......... .......... 20%  257K 3s\n",
      "   100K .......... .......... .......... .......... .......... 31%  130K 3s\n",
      "   150K .......... .......... .......... .......... .......... 41% 27.9M 2s\n",
      "   200K .......... .......... .......... .......... .......... 51%  259K 1s\n",
      "   250K .......... .......... .......... .......... .......... 62% 46.1M 1s\n",
      "   300K .......... .......... .......... .......... .......... 72%  262K 1s\n",
      "   350K .......... .......... .......... .......... .......... 82% 20.0M 0s\n",
      "   400K .......... .......... .......... .......... .......... 93% 50.0M 0s\n",
      "   450K .......... .......... .......... ..                   100%  171K=1.7s\n",
      "\n",
      "2020-11-19 18:33:37 (277 KB/s) - ‘S1A_AUX_CAL_V20140908T000000_G20190626T100201.SAFE.TGZ’ saved [493780/493780]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "wget https://qc.sentinel1.eo.esa.int/product/S1A/AUX_CAL/20140908T000000/S1A_AUX_CAL_V20140908T000000_G20190626T100201.SAFE.TGZ\n",
    "tar zxvf S1A_AUX_CAL_V20140908T000000_G20190626T100201.SAFE.TGZ\n",
    "rm S1A_AUX_CAL_V20140908T000000_G20190626T100201.SAFE.TGZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The scripts provides support for Sentinel-1 TOPS stack processing. Currently supported workflows include a coregistered stack of SLC, interferograms, offsets, and coherence. \n",
    "\n",
    "`stackSentinel.py` generates all configuration and run files required to be executed on a stack of Sentinel-1 TOPS data. When stackSentinel.py is executed for a given workflow (-W option) a **configs** and **run_files** folder is generated. No processing is performed at this stage. Within the run_files folder different run\\_#\\_description files are contained which are to be executed as shell scripts in the run number order. Each of these run scripts call specific configure files contained in the “configs” folder which call ISCE in a modular fashion. The configure and run files will change depending on the selected workflow. To make run_# files executable, change the file permission accordingly (e.g., `chmod +x run_01_unpack_slc`).\n",
    "\n",
    "```bash\n",
    "stackSentinel.py -H     #To see workflow examples,\n",
    "stackSentinel.py -h     #To get an overview of all the configurable parameters\n",
    "```\n",
    "\n",
    "Required parameters of stackSentinel.py include:\n",
    "\n",
    "```cfg\n",
    "-s SLC_DIRNAME          #A folder with downloaded Sentinel-1 SLC’s. \n",
    "-o ORBIT_DIRNAME        #A folder containing the Sentinel-1 orbits. Missing orbit files will be downloaded automatically\n",
    "-a AUX_DIRNAME          #A folder containing the Sentinel-1 Auxiliary files\n",
    "-d DEM_FILENAME         #A DEM (Digital Elevation Model) referenced to wgs84\n",
    "```\n",
    "\n",
    "In the following, different workflow examples are provided. Note that stackSentinel.py only generates the run and configure files. To perform the actual processing, the user will need to execute each run file in their numbered order.\n",
    "\n",
    "In all workflows, coregistration (-C option) can be done using only geometry (set option = geometry) or with geometry plus refined azimuth offsets through NESD (set option = NESD) approach, the latter being the default. For the NESD coregistrstion the user can control the ESD coherence threshold (-e option) and the number of overlap interferograms (-O) to be used in NESD estimation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "master_date : \n",
      "MASTER_DATE DOES NOT EXIST\n",
      "Calling : /opt/isce2/src/isce2/contrib/stack/topsStack/stackSentinel.py -s zip/ -d /home/jovyan/nisar-on-demand-use-cases/demLat_N34_N35_Lon_W080_W078.dem.wgs84 -a AuxDir/ -o Orbits -b \"34.6002832 34.6502392 -79.0801608 -78.9705888\" -W slc -C geometry\n",
      "b'Using default ISCE Path: /opt/isce2/isce'\n",
      "b'Number of SAFE files found: 5'\n",
      "b'*****************************************'\n",
      "b'20160414'\n",
      "b'orbit was not found in the /home/jovyan/nisar-on-demand-use-cases/Orbits'\n",
      "b'downloading precise or restituted orbits ...'\n",
      "b'*****************************************'\n",
      "b'20150830'\n",
      "b'orbit was not found in the /home/jovyan/nisar-on-demand-use-cases/Orbits'\n",
      "b'downloading precise or restituted orbits ...'\n",
      "b'restituted orbit already exists.'\n",
      "b'*****************************************'\n",
      "b'20150818'\n",
      "b'orbit was not found in the /home/jovyan/nisar-on-demand-use-cases/Orbits'\n",
      "b'downloading precise or restituted orbits ...'\n",
      "b'*****************************************'\n",
      "b'20160414'\n",
      "b'orbit was not found in the /home/jovyan/nisar-on-demand-use-cases/Orbits'\n",
      "b'downloading precise or restituted orbits ...'\n",
      "b'*****************************************'\n",
      "b'20150315'\n",
      "b'orbit was not found in the /home/jovyan/nisar-on-demand-use-cases/Orbits'\n",
      "b'downloading precise or restituted orbits ...'\n",
      "b'restituted orbit already exists.'\n",
      "b'Number of SAFE files to be used (cover BBOX): 5'\n",
      "b'*****************************************'\n",
      "b'Number of dates : 4'\n",
      "b'List of dates :'\n",
      "b\"['20150315', '20150818', '20150830', '20160414']\"\n",
      "b'date      south      north'\n",
      "b'20150315 33.004826 35.203526'\n",
      "b'20150818 33.004375 35.19812'\n",
      "b'20150830 33.336544 35.529255'\n",
      "b'20160414 32.673004 36.409657'\n",
      "b'*****************************************'\n",
      "b'The overlap region among all dates (based on the preview kml files):'\n",
      "b'South   North   East  West'\n",
      "b'33.336544 35.19812 -79.956169 -76.943398'\n",
      "b'*****************************************'\n",
      "b'All dates (4)'\n",
      "b\"['20150315', '20150818', '20150830', '20160414']\"\n",
      "b''\n",
      "b'dates covering the bbox (4)'\n",
      "b\"['20150315', '20150818', '20150830', '20160414']\"\n",
      "b''\n",
      "b'The reference date was not chosen. The first date is considered as reference date.'\n",
      "b''\n",
      "b'All SLCs will be coregistered to : 20150315'\n",
      "b'secondary dates :'\n",
      "b\"['20150818', '20150830', '20160414']\"\n",
      "b''\n",
      "b'No existing stack was identified. A new stack will be generated.'\n",
      "b'*****************************************'\n",
      "b'Coregistration method:  geometry'\n",
      "b'Workflow:  slc'\n",
      "b'*****************************************'\n",
      "b'writing  /home/jovyan/nisar-on-demand-use-cases/run_files/run_01_unpack_topo_reference'\n",
      "b'writing  /home/jovyan/nisar-on-demand-use-cases/run_files/run_02_unpack_secondary_slc'\n"
     ]
    }
   ],
   "source": [
    "master_date=get_master_date(ctx)\n",
    "print(\"master_date : {}\".format(master_date))\n",
    "\n",
    "if master_date:\n",
    "    print(\"MASTER_DATE exists:\".format(master_date) )   \n",
    "    cmd = [\n",
    "        \"/opt/isce2/src/isce2/contrib/stack/topsStack/stackSentinel.py\",  \"-s\", \"zip/\", \"-d\", \"{}\".format(wgs84_file), \"-a\", \"AuxDir/\", \"-m\", \"{}\".format(master_date), \"-o\", \"Orbits\", \n",
    "        \"-b\", \"\\\"{} {} {} {}\\\"\".format(MINLAT, MAXLAT, MINLON, MAXLON), \n",
    "        \"-W\", \"slc\", \"-C\", \"geometry\"\n",
    "    ]\n",
    "else:\n",
    "    print(\"MASTER_DATE DOES NOT EXIST\")          \n",
    "    cmd = [\n",
    "        \"/opt/isce2/src/isce2/contrib/stack/topsStack/stackSentinel.py\",  \"-s\", \"zip/\", \"-d\", \"{}\".format(wgs84_file), \"-a\", \"AuxDir/\", \"-o\", \"Orbits\", \n",
    "        \"-b\", \"\\\"{} {} {} {}\\\"\".format(MINLAT, MAXLAT, MINLON, MAXLON), \n",
    "        \"-W\", \"slc\", \"-C\", \"geometry\"\n",
    "    ]\n",
    "run_cmd(cmd)          \n",
    "              \n",
    "          \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling : topsstack-hamsar/topsStack/stackSlcDn_run2.5.sh 34.6002832 34.6502392 -79.0801608 -78.9705888\n"
     ]
    }
   ],
   "source": [
    "cmd = [\"topsstack-hamsar/topsStack/stackSlcDn_run2.5.sh\", \"{} {} {} {}\".format(MINLAT, MAXLAT, MINLON, MAXLON)]\n",
    "run_cmd(cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**run_01_unpack_slc_topo_reference:**\n",
    "\n",
    "Includes commands to unpack Sentinel-1 TOPS SLCs using ISCE readers. For older SLCs which need antenna elevation pattern correction, the file is extracted and written to disk. For newer version of SLCs which don’t need the elevation antenna pattern correction, only a gdal virtual “vrt” file (and isce xml file) is generated. The “.vrt” file points to the Sentinel SLC file and reads them whenever required during the processing. If a user wants to write the “.vrt” SLC file to disk, it can be done easily using gdal_translate (e.g. gdal_translate –of ENVI File.vrt File.slc). \n",
    "The “run_01_unpack_slc_topo_reference” also includes a command that refers to the config file of the stack reference, which includes configuration for running topo for the stack reference. Note that in the pair-wise processing strategy one should run topo (mapping from range-Doppler to geo coordinate) for all pairs. However, with stackSentinel, topo needs to be run only one time for the reference in the stack. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "runtime_dict[\"s1_start\"]=get_current_time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "source /opt/conda/etc/profile.d/conda.sh\n",
    "conda activate base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "source /opt/isce2/isce_env.sh\n",
    "source /opt/conda/etc/profile.d/conda.sh\n",
    "export PATH=/opt/conda/bin/:/opt/isce2/src/isce2/contrib/stack/topsStack/:${PATH}\n",
    "export PYTHONPATH=/opt/isce2/src/isce2/contrib/stack/topsStack/:${PYTHONPATH}\n",
    "conda activate base\n",
    "start=`date +%s`\n",
    "echo \"sh run_files/run_01_unpack_topo_reference\"\n",
    "sh run_files/run_01_unpack_topo_reference\n",
    "end=`date +%s`\n",
    "runtime1=$((end-start))\n",
    "echo $runtime1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'s1_start': datetime.datetime(2020, 11, 19, 18, 40, 52, 601587), 's1_stop': datetime.datetime(2020, 11, 19, 18, 41, 1, 606110), 's1_runtime': datetime.timedelta(seconds=9, microseconds=4523)}\n"
     ]
    }
   ],
   "source": [
    "runtime_dict[\"s1_stop\"]=get_current_time()\n",
    "runtime_dict[\"s1_runtime\"]=runtime_dict[\"s1_stop\"]-runtime_dict[\"s1_start\"]\n",
    "print(runtime_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**run_02_average_baseline:**\n",
    "\n",
    "Computes average baseline for the stack. These baselines are not used for processing anywhere. They are only an approximation and can be used for plotting purposes. A more precise baseline grid is estimated later in run_10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 2\n",
      "0\n",
      "cat run_files/run_02_unpack_secondary_slc | parallel -j2 --eta --load 50%\n",
      "runtime2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sh: /dev/tty: No such device or address\n",
      "\n",
      "Computers / CPU cores / Max jobs to run\n",
      "1:local / 36 / 1\n",
      "ETA: 0s Left: 0 AVG: 0.00s  0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%%bash\n",
    "echo \"STEP 2\"\n",
    "source /opt/isce2/isce_env.sh\n",
    "export PATH=/opt/conda/bin/:/opt/isce2/src/isce2/contrib/stack/topsStack/:${PATH}\n",
    "export PYTHONPATH=/opt/isce2/src/isce2/contrib/stack/topsStack/:${PYTHONPATH}\n",
    "start=`date +%s`\n",
    "Num=`cat run_files/run_02_unpack_secondary_slc | wc | awk '{print $1}'`\n",
    "echo $Num\n",
    "echo \"cat run_files/run_02_unpack_secondary_slc | parallel -j2 --eta --load 50%\"\n",
    "cat run_files/run_02_unpack_secondary_slc | parallel -j2 --eta --load 50%\n",
    "end=`date +%s`\n",
    "\n",
    "runtime2=$((end-start))\n",
    "echo runtime2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runtime_dict[\"s2_stop\"]=get_current_time()\n",
    "runtime_dict[\"s2_runtime\"]=runtime_dict[\"s2_stop\"]-runtime_dict[\"s1_stop\"]\n",
    "print(runtime_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run_02.5_slc_noise_calibration ###\n",
    "STEP 2.5 run radiometric and thermal noise calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%%bash\n",
    "echo \"STEP 2.5\"\n",
    "source /opt/isce2/isce_env.sh\n",
    "export PATH=/opt/conda/bin/:/opt/isce2/src/isce2/contrib/stack/topsStack/:~/topsstack-hamsar/topsStack:~/topsstack-hamsar/:${PATH}\n",
    "export PYTHONPATH=/opt/isce2/src/isce2/contrib/stack/topsStack/:~/topsstack-hamsar/topsStack:~/topsstack-hamsar/:${PYTHONPATH}\n",
    "start=`date +%s`\n",
    "echo \"cat run_files/run_02.5_slc_noise_calibration | parallel -j2 --eta --load 50%\"\n",
    "cat run_files/run_02.5_slc_noise_calibration | parallel -j2 --eta --load 50%\n",
    "end=`date +%s`\n",
    "\n",
    "runtime2x5=$((end-start))\n",
    "echo $runtime2x5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runtime_dict[\"s2.5_stop\"]=get_current_time()\n",
    "runtime_dict[\"s2.5_runtime\"]=runtime_dict[\"s2.5_stop\"]-runtime_dict[\"s2_stop\"]\n",
    "print(runtime_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3 : run_03_extract_burst_overlaps:**\n",
    "\n",
    "Burst overlaps are extracted for estimating azimuth misregistration using NESD technique. If coregistration method is chosen to be “geometry”, then this run file won’t exist and the overlaps are not extracted.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "echo \"STEP 3\"\n",
    "source /opt/isce2/isce_env.sh\n",
    "export PATH=/opt/conda/bin/:/opt/isce2/src/isce2/contrib/stack/topsStack/:~/topsstack-hamsar/topsStack:~/topsstack-hamsar/:${PATH}\n",
    "export PYTHONPATH=/opt/isce2/src/isce2/contrib/stack/topsStack/:~/topsstack-hamsar/topsStack:~/topsstack-hamsar/:${PYTHONPATH}\n",
    "start=`date +%s`\n",
    "echo \"cat run_files/run_03_average_baseline | parallel -j2 --eta --load 50%\"\n",
    "cat run_files/run_03_average_baseline | parallel -j2 --eta --load 50%\n",
    "end=`date +%s`\n",
    "runtime3=$((end-start))\n",
    "echo $runtime3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runtime_dict[\"s3_stop\"]=get_current_time()\n",
    "runtime_dict[\"s3_runtime\"]=runtime_dict[\"s3_stop\"]-runtime_dict[\"s2.5_stop\"]\n",
    "print(runtime_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STEP 4 : run_04_overlap_geo2rdr_resample:***\n",
    "\n",
    "Running geo2rdr to estimate geometrical offsets between secondary burst overlaps and the stack reference burst overlaps. The secondary burst overlaps are then resampled to the stack reference burst overlaps. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "echo \"## STEP 4 ##\"\n",
    "source /opt/isce2/isce_env.sh\n",
    "export PATH=/opt/conda/bin/:/opt/isce2/src/isce2/contrib/stack/topsStack/:~/topsstack-hamsar/topsStack:~/topsstack-hamsar/:${PATH}\n",
    "export PYTHONPATH=/opt/isce2/src/isce2/contrib/stack/topsStack/:~/topsstack-hamsar/topsStack:~/topsstack-hamsar/:${PYTHONPATH}\n",
    "start=`date +%s`\n",
    "\n",
    "echo \"cat run_files/run_04_fullBurst_geo2rdr  | parallel -j2 --eta --load 50%\"\n",
    "cat run_files/run_04_fullBurst_geo2rdr  | parallel -j2 --eta --load 50%\n",
    "end=`date +%s`\n",
    "runtime4=$((end-start))\n",
    "echo $runtime4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runtime_dict[\"s4_stop\"]=get_current_time()\n",
    "runtime_dict[\"s4_runtime\"]=runtime_dict[\"s4_stop\"]-runtime_dict[\"s3_stop\"]\n",
    "print(runtime_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STEP 5 : run_05_pairs_misreg:**\n",
    "\n",
    "Using the coregist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "## STEP 5 ##\n",
    "source /opt/isce2/isce_env.sh\n",
    "export PATH=/opt/conda/bin/:/opt/isce2/src/isce2/contrib/stack/topsStack/:~/topsstack-hamsar/topsStack:~/topsstack-hamsar/:${PATH}\n",
    "export PYTHONPATH=/opt/isce2/src/isce2/contrib/stack/topsStack/:~/topsstack-hamsar/topsStack:~/topsstack-hamsar/:${PYTHONPATH}\n",
    "start=`date +%s`\n",
    "echo \"cat run_files/run_05_fullBurst_resample  | parallel -j2 --eta --load 50%\"\n",
    "cat run_files/run_05_fullBurst_resample  | parallel -j2 --eta --load 50%\n",
    "end=`date +%s`\n",
    "runtime5=$((end-start))\n",
    "echo $runtime5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runtime_dict[\"s5_stop\"]=get_current_time()\n",
    "runtime_dict[\"s5_runtime\"]=runtime_dict[\"s5_stop\"]-runtime_dict[\"s4_stop\"]\n",
    "print(runtime_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STEP 6 : run_06_timeseries_misreg:**\n",
    "\n",
    "A time-series of azimuth and range misregistration is estimated with respect to the stack reference. The time-series is a least squares esatimation from the pair misregistration from the previous step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "source /opt/isce2/isce_env.sh\n",
    "export PATH=/opt/conda/bin/:/opt/isce2/src/isce2/contrib/stack/topsStack/:${PATH}\n",
    "export PYTHONPATH=/opt/isce2/src/isce2/contrib/stack/topsStack/:${PYTHONPATH}\n",
    "echo \"# STEP 6 ##\"\n",
    "start=`date +%s`\n",
    "echo \"sh run_files/run_06_extract_stack_valid_region\"\n",
    "echo $start\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runtime_dict[\"s6_stop\"]=get_current_time()\n",
    "runtime_dict[\"s6_runtime\"]=runtime_dict[\"s6_stop\"]-runtime_dict[\"s5_stop\"]\n",
    "print(runtime_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**run_07_geo2rdr_resample:**\n",
    "\n",
    "Using orbit and DEM, geometrical offsets among all secondary SLCs and the stack reference is computed. The goometrical offsets, together with the misregistration time-series (from previous step) are used for precise coregistration of each burst SLC. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "source /opt/isce2/isce_env.sh\n",
    "export PATH=/opt/conda/bin/:/opt/isce2/src/isce2/contrib/stack/topsStack/:${PATH}\n",
    "export PYTHONPATH=/opt/isce2/src/isce2/contrib/stack/topsStack/:${PYTHONPATH}\n",
    "start=`date +%s`\n",
    "echo \"cat run_files/run_07_merge  | parallel -j2 --eta --load 50%\"\n",
    "cat run_files/run_07_merge | parallel -j2 --eta --load 50%\n",
    "end=`date +%s`\n",
    "runtime7=$((end-start))\n",
    "echo $runtime7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runtime_dict[\"s7_stop\"]=get_current_time()\n",
    "runtime_dict[\"s7_runtime\"]=runtime_dict[\"s7_stop\"]-runtime_dict[\"s6_stop\"]\n",
    "print(runtime_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"time before creating dataset : {}\".format(get_current_time()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python topsstack-hamsar/create_dataset.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"time after creating dataset : {}\".format(get_current_time()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install opencv-python-headless"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
