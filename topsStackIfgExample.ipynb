{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"1\">Copyright 2021, by the California Institute of Technology. ALL RIGHTS RESERVED. United States Government sponsorship acknowledged. Any commercial use must be negotiated with the Office of Technology Transfer at the California Institute of Technology.</font>\n",
    "    \n",
    "<font size=\"1\">This software may be subject to U.S. export control laws and regulations. By accepting this document, the user agrees to comply with all applicable U.S. export laws and regulations. User has the responsibility to obtain export licenses, or other export authority as may be required, before exporting such information to foreign countries or providing access to foreign persons.<font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Sentinel-1 TOPS stack processor\n",
    "The detailed algorithm for stack processing of TOPS data can be find here:\n",
    "\n",
    "+ Fattahi, H., P. Agram, and M. Simons (2016), A Network-Based Enhanced Spectral Diversity Approach for TOPS Time-Series Analysis, IEEE Transactions on Geoscience and Remote Sensing, 55(2), 777-786, doi:[10.1109/TGRS.2016.2614925](https://ieeexplore.ieee.org/abstract/document/7637021).\n",
    "\n",
    "-----------------------------------\n",
    "\n",
    "The scripts provides support for Sentinel-1 TOPS stack processing. Currently supported workflows include a coregistered stack of SLC, interferograms, offsets, and coherence. \n",
    "\n",
    "\n",
    "##To use the sentinel stack processor, make sure to add the path of your `contrib/stack/topsStack` folder to your `$PATH` environment varibale. \n",
    "\n",
    "#### Be sure [default] credentials in ~/.aws/credentials are valid\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Begin by loading required libraries and dependencies - already installed in the on-demand system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from math import floor, ceil\n",
    "import json\n",
    "\n",
    "import osaka\n",
    "import osaka.main\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import gdal\n",
    "from builtins import str\n",
    "import os, sys, re, json, logging, traceback, requests, argparse\n",
    "from datetime import datetime\n",
    "from pprint import pformat\n",
    "from matplotlib import pyplot as plt\n",
    "from requests.packages.urllib3.exceptions import (InsecureRequestWarning,\n",
    "                                                  InsecurePlatformWarning)\n",
    "try: from html.parser import HTMLParser\n",
    "except: from html.parser import HTMLParser\n",
    "        \n",
    "requests.packages.urllib3.disable_warnings(InsecureRequestWarning)\n",
    "requests.packages.urllib3.disable_warnings(InsecurePlatformWarning)\n",
    "\n",
    "# this block makes sure the directory set-up/change is only done once and relative to the notebook's directory\\n\",\n",
    "try:\n",
    "    start_dir\n",
    "except NameError:\n",
    "    start_dir = os.getcwd()\n",
    "    output_dir = os.path.join(start_dir, 'notebook_output/topsStackIfgExample')\n",
    "    work_dir = os.path.join(output_dir, 'LVC_stack_small')\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "os.chdir(output_dir)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "SLC_RE = re.compile(r'(?P<mission>S1\\w)_IW_SLC__.*?' +\n",
    "                    r'_(?P<start_year>\\d{4})(?P<start_month>\\d{2})(?P<start_day>\\d{2})' +\n",
    "                    r'T(?P<start_hour>\\d{2})(?P<start_min>\\d{2})(?P<start_sec>\\d{2})' +\n",
    "                    r'_(?P<end_year>\\d{4})(?P<end_month>\\d{2})(?P<end_day>\\d{2})' +\n",
    "                    r'T(?P<end_hour>\\d{2})(?P<end_min>\\d{2})(?P<end_sec>\\d{2})_.*$')\n",
    "ISCE_HOME=\"/opt/isce2/isce\"\n",
    "\n",
    "ORBITMAP = [('precise','aux_poeorb', 100),\n",
    "            ('restituted','aux_resorb', 100)]\n",
    "\n",
    "OPER_RE = re.compile(r'S1\\w_OPER_AUX_(?P<type>\\w+)_OPOD_(?P<yr>\\d{4})(?P<mo>\\d{2})(?P<dy>\\d{2})')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Define your dataset and a few processing parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "global runtime_dict\n",
    "runtime_dict = {}\n",
    "ISCE_HOME=\"/opt/isce2/isce\"\n",
    "ctx = {}\n",
    "\n",
    "ctx[\"min_lat\"] = \"\"\n",
    "ctx[\"max_lat\"] = \"\"\n",
    "ctx[\"min_lon\"] = \"\"\n",
    "ctx[\"max_lon\"] = \"\"\n",
    "\n",
    "ctx[\"master_date\"]=\"20180103\"\n",
    "ctx[\"localize_slcs\"]= [\"S1A_IW_SLC__1SDV_20180103T135924_20180103T135951_019991_0220D6_3C80\", \n",
    "                       \"S1A_IW_SLC__1SDV_20180421T135924_20180421T135951_021566_0252AE_60EA\",\n",
    "                       \"S1A_IW_SLC__1SDV_20180714T135929_20180714T135956_022791_027880_9FCA\",\n",
    "                       \"S1A_IW_SLC__1SDV_20181018T135933_20181018T140000_024191_02A573_AEBB\",\n",
    "                       \"S1A_IW_SLC__1SDV_20190110T135931_20190110T135958_025416_02D0B7_C555\",\n",
    "                       \"S1A_IW_SLC__1SDV_20190416T135930_20190416T135957_026816_03038D_4177\",\n",
    "                       \"S1A_IW_SLC__1SDV_20190721T135936_20190721T140003_028216_032FFD_1F8B\",\n",
    "                       \"S1A_IW_SLC__1SDV_20191013T135939_20191013T140006_029441_035951_9DBD\",\n",
    "                       \"S1A_IW_SLC__1SDV_20191212T135938_20191212T140005_030316_0377AE_ED8B\"\n",
    "                     ]\n",
    "ctx[\"work_dir\"] = work_dir\n",
    "wd = ctx[\"work_dir\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Load some helpful modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "import urllib.parse\n",
    "\n",
    "def session_get(session, url):\n",
    "    return session.get(url, verify=False)\n",
    "\n",
    "datefmt = \"%Y%m%dT%H%M%S\"\n",
    "queryfmt = \"%Y-%m-%d\"\n",
    "server = 'https://scihub.copernicus.eu/gnss/'\n",
    "auth_netloc_fmt = \"{}:{}@{}\"\n",
    "url_tpt = 'search?q=( beginPosition:[{0}T00:00:00.000Z TO {1}T23:59:59.999Z] AND endPosition:[{0}T00:00:00.000Z TO {1}T23:59:59.999Z] ) AND ( (platformname:Sentinel-1 AND filename:{2}_* AND producttype:{3}))&start=0&rows=100'\n",
    "credentials = ('gnssguest','gnssguest')\n",
    "\n",
    "class MyHTMLParser(HTMLParser):\n",
    "\n",
    "    def __init__(self,url):\n",
    "        HTMLParser.__init__(self)\n",
    "        self.fileList = []\n",
    "        self._url = url\n",
    "\n",
    "    def handle_starttag(self, tag, attrs):\n",
    "        for name, val in attrs:\n",
    "            if name == 'href':\n",
    "                if val.startswith(\"https://scihub.copernicus.eu/gnss/odata\") and val.endswith(\")/\"):\n",
    "                    pass\n",
    "                else:\n",
    "                    downloadLink = val.strip()\n",
    "                    downloadLink = downloadLink.split(\"/Products('Quicklook')\")\n",
    "                    downloadLink = downloadLink[0] + downloadLink[-1]\n",
    "                    self._url = downloadLink\n",
    "\n",
    "    def handle_data(self, data):\n",
    "        if data.startswith(\"S1\") and data.endswith(\".EOF\"):\n",
    "            self.fileList.append((self._url, data.strip()))\n",
    "\n",
    "\n",
    "def fileToRange(fname):\n",
    "    '''\n",
    "    Derive datetime range from orbit file name.\n",
    "    '''\n",
    "\n",
    "    fields = os.path.basename(fname).split('_')\n",
    "    start = datetime.strptime(fields[-2][1:16], datefmt)\n",
    "    stop = datetime.strptime(fields[-1][:15], datefmt)\n",
    "    mission = fields[0]\n",
    "\n",
    "    return (start, stop, mission)\n",
    "\n",
    "\n",
    "def get_download_orbit_dict(slc_start_dt, slc_end_dt, sat_name):\n",
    "\n",
    "    found = False\n",
    "    delta = timedelta(days=1)\n",
    "    timebef = (slc_end_dt - delta).strftime(queryfmt)\n",
    "    timeaft = (slc_end_dt + delta).strftime(queryfmt)\n",
    "    match = None\n",
    "    matchFileName = None\n",
    "    \n",
    "    session = requests.Session()\n",
    "\n",
    "    for fidelity in ('AUX_POEORB', 'AUX_RESORB'):\n",
    "        url = server + url_tpt.format(timebef, timeaft, sat_name, fidelity)\n",
    "        # print(f\"url: {url}\")\n",
    "\n",
    "        try:\n",
    "            r = session.get(url, verify=True, auth=credentials)\n",
    "            r.raise_for_status()\n",
    "            parser = MyHTMLParser(url)\n",
    "            parser.feed(r.text)\n",
    "\n",
    "            for resulturl, result in parser.fileList:\n",
    "                # print('Results: {} : {}'.format(resulturl, result))\n",
    "                tbef, taft, mission = fileToRange(os.path.basename(result))\n",
    "\n",
    "                if (tbef <= slc_start_dt) and (taft >= slc_end_dt):\n",
    "                    matchFileName = result\n",
    "                    parse_url = urllib.parse.urlsplit(resulturl)\n",
    "                    # Add credentials for osaka\n",
    "                    new_netloc = auth_netloc_fmt.format(credentials[0],credentials[1],parse_url.netloc)\n",
    "                    match = urllib.parse.urlunsplit(parse_url._replace(netloc=new_netloc))\n",
    "                else:\n",
    "                    print(\"no match.\")\n",
    "            \n",
    "            if match is not None:\n",
    "                found = True\n",
    "        except Exception as e:\n",
    "            print(\"Exception {}\".format(e))\n",
    "        \n",
    "        if found:\n",
    "            break\n",
    "\n",
    "    # print(\"returning {} : {}\".format(matchFileName, match))\n",
    "    return matchFileName, match\n",
    "\n",
    "\n",
    "def get_orbit_files():\n",
    "    import json\n",
    "    import os\n",
    "    import osaka\n",
    "    import osaka.main\n",
    "    orbit_dates = []\n",
    "        \n",
    "    for slc in ctx[\"localize_slcs\"]:\n",
    "        match = SLC_RE.search(slc)\n",
    "        if not match:\n",
    "            raise RuntimeError(\"Failed to recognize SLC ID %s.\" %slc)\n",
    "        mission = match.group('mission')\n",
    "        slc_start_dt_str = \"{}-{}-{}T{}:{}:{}\".format(match.group('start_year'), \n",
    "                                                      match.group('start_month'),\n",
    "                                                      match.group('start_day'),\n",
    "                                                      match.group('start_hour'),\n",
    "                                                      match.group('start_min'),\n",
    "                                                      match.group('start_sec'))\n",
    "        slc_start_dt = datetime.strptime(slc_start_dt_str, \"%Y-%m-%dT%H:%M:%S\")\n",
    "        slc_end_dt_str = \"{}-{}-{}T{}:{}:{}\".format(match.group('end_year'), \n",
    "                                                      match.group('end_month'),\n",
    "                                                      match.group('end_day'),\n",
    "                                                      match.group('end_hour'),\n",
    "                                                      match.group('end_min'),\n",
    "                                                      match.group('end_sec'))\n",
    "        slc_end_dt = datetime.strptime(slc_end_dt_str, \"%Y-%m-%dT%H:%M:%S\")\n",
    "        day_dt_str = slc_start_dt.strftime('%Y-%m-%d')\n",
    "        if day_dt_str not in orbit_dates:\n",
    "            orbit_dates.append(day_dt_str)\n",
    "            orbit_file_name, orbit_file_uri = get_download_orbit_dict(slc_start_dt, slc_end_dt, mission)\n",
    "            if orbit_file_uri is not None:\n",
    "                directory = os.path.join(wd, \"orbits\")\n",
    "                if not os.path.exists(directory):\n",
    "                    os.makedirs(directory)\n",
    "                    print(f\"created {directory}\")\n",
    "                osaka.main.get(orbit_file_uri, os.path.join(directory, orbit_file_name))\n",
    "                print(\"Downloaded orbit file {}\".format(orbit_file_name))\n",
    "            else:\n",
    "                print('No orbit files found for slc: {}'.format(slc))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def get_current_time():\n",
    "    import datetime\n",
    "    return datetime.datetime.now()\n",
    "\n",
    "def download_slc(slc_id, path):\n",
    "    \n",
    "    \n",
    "    url = \"https://datapool.asf.alaska.edu/SLC/SA/{}.zip\".format(slc_id)\n",
    "    print(\"Downloading {} : {}\".format(slc_id, url))\n",
    "    \n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    osaka.main.get(url, path)\n",
    "\n",
    "def run_cmd_output(cmd):\n",
    "    from subprocess import check_output, CalledProcessError\n",
    "    cmd_line = \" \".join(cmd)\n",
    "    print(\"Calling: {}\".format(cmd_line))\n",
    "    output = check_output(cmd_line, shell=True)\n",
    "    return output\n",
    "\n",
    "def run_cmd(cmd):\n",
    "\n",
    "    import subprocess\n",
    "    from subprocess import check_call, CalledProcessError\n",
    "    import sys\n",
    "    cmd_line = \" \".join(cmd)\n",
    "    print(\"Calling : {}\".format(cmd_line))\n",
    "    p = subprocess.Popen(cmd_line, shell=True,stdout=subprocess.PIPE,stderr=subprocess.PIPE)\n",
    "    while True: \n",
    "        line = p.stdout.readline()\n",
    "        if not line:\n",
    "            break\n",
    "        print(line.strip())\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "def get_minimum_bounding_rectangle(ctx):\n",
    "    from math import floor, ceil\n",
    "\n",
    "    slc_ids = [x for x in os.listdir(os.path.join(ctx[\"work_dir\"],'zip')) if '.zip' in x and '_SLC__' in x]\n",
    "    #print(slc_ids)\n",
    "    \n",
    "    all_north = []\n",
    "    all_south = []\n",
    "    all_west = []\n",
    "    all_east = []\n",
    "    \n",
    "    for slc in slc_ids:\n",
    "        \n",
    "        zipped = zipfile.ZipFile(os.path.join(wd,'zip',slc), \"r\")\n",
    "        overlay = os.path.join(slc.split('.')[0] + '.SAFE', 'preview', 'map-overlay.kml')\n",
    "        overlay_contents = str(zipped.read(overlay)).split('\\\\n')\n",
    "\n",
    "\n",
    "        date_lats = []\n",
    "        date_lons = []\n",
    "        for line in overlay_contents:\n",
    "\n",
    "            if 'coordinates' in line:\n",
    "                coords_str = line.strip().split('>')[1].split('<')[0]\n",
    "                coords_list = coords_str.split()\n",
    "            \n",
    "                for pair in coords_list:\n",
    "\n",
    "                    date_lats.append(float(pair.split(',')[1]))\n",
    "                    date_lons.append(float(pair.split(',')[0]))\n",
    "        #print()\n",
    "        #print(date_lats)\n",
    "        #print(date_lons)\n",
    "        all_south.append(min(date_lats))\n",
    "        all_north.append(max(date_lats))\n",
    "        all_west.append(min(date_lons))\n",
    "        all_east.append(max(date_lons))\n",
    "    #print()\n",
    "    #print(all_lats)\n",
    "    #print(all_lons)\n",
    "    min_lat = max(all_south)\n",
    "    max_lat = min(all_north) \n",
    "    min_lon = max(all_west)\n",
    "    max_lon = min(all_east)\n",
    "\n",
    "    min_lat_lo = floor(min_lat)\n",
    "    max_lat_hi = ceil(max_lat)\n",
    "    min_lon_lo = floor(min_lon)\n",
    "    max_lon_hi = ceil(max_lon)\n",
    "\n",
    "    return min_lat, max_lat, min_lon, max_lon, min_lat_lo, max_lat_hi, min_lon_lo, max_lon_hi\n",
    "\n",
    "def get_user_input_bbox(ctx):\n",
    "    \"\"\"\n",
    "    :param ctx_file: dictionary from cxt file\n",
    "    :return: void\n",
    "    \"\"\"\n",
    "    from math import floor, ceil\n",
    "    min_lat = ctx_file['min_lat']\n",
    "    max_lat = ctx_file['max_lat']\n",
    "    min_lon = ctx_file['min_lon']\n",
    "    max_lon = ctx_file['max_lon']\n",
    "\n",
    "    min_lat_lo = floor(min_lat)\n",
    "    max_lat_hi = ceil(max_lat)\n",
    "    min_lon_lo = floor(min_lon)\n",
    "    max_lon_hi = ceil(max_lon)\n",
    "\n",
    "    return min_lat, max_lat, min_lon, max_lon, min_lat_lo, max_lat_hi, min_lon_lo, max_lon_hi\n",
    "\n",
    "def get_master_date(ctx):\n",
    "    master_date = ctx.get('master_date', \"\")\n",
    "    return master_date\n",
    "\n",
    "def get_bbox(ctx):\n",
    "    # min_lat, max_lat, min_lon, max_lon = ctx['region_of_interest']\n",
    "\n",
    "    if ctx['min_lat'] != \"\" and ctx['max_lat'] != \"\" and ctx['min_lon'] != \"\" and ctx['max_lon'] != \"\":\n",
    "        # if all values are present in _context.json we can assume user put them in manually\n",
    "        bbox_data = get_user_input_bbox(ctx)\n",
    "    else:\n",
    "        # if user did not define ANY lat lons\n",
    "        bbox_data = get_minimum_bounding_rectangle(ctx)\n",
    "        #print(bbox_data)\n",
    "\n",
    "    return bbox_data\n",
    "\n",
    "def download_dem():\n",
    "    dem_cmd = [\n",
    "        \"{}/applications/dem.py\".format(ISCE_HOME), \"-a\",\n",
    "        \"stitch\", \"-b\", \"{} {} {} {}\".format(MINLAT_LO, MAXLAT_HI, MINLON_LO, MAXLON_HI),\n",
    "        \"-r\", \"-s\", \"1\", \"-f\", \"-c\", \"|\", \"tee\", \"dem.txt\"\n",
    "        #\"-n\", dem_user, \"-w\", dem_pass,\"-u\", dem_url\n",
    "    ]\n",
    "    run_cmd(dem_cmd)\n",
    "    \n",
    "def download_slcs(zipdir):\n",
    "    \n",
    "    for slc in ctx[\"localize_slcs\"]:\n",
    "        slcfile = os.path.join(wd,zipdir,slc+'.zip')\n",
    "        if not os.path.exists(slcfile):\n",
    "            print('Downloading ', slcfile)\n",
    "            download_slc(slc, os.path.join(wd,zipdir))\n",
    "        else:\n",
    "            print('Found ', slcfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "## Download Sentinel-1 data SLCs ####\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "zipdir = \"zip\"\n",
    "download_slcs(zipdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Find a common area of interest from all listed images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "MINLAT, MAXLAT, MINLON, MAXLON, MINLAT_LO, MAXLAT_HI, MINLON_LO, MAXLON_HI =get_bbox(ctx)\n",
    "print(\"{} {} {} {} {} {} {} {}\".format(MINLAT, MAXLAT, MINLON, MAXLON, MINLAT_LO, MAXLAT_HI, MINLON_LO, MAXLON_HI))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Visualize the area of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "bbox_poly = [(MINLAT, MINLON),(MINLAT, MAXLON),(MAXLAT,MAXLON),(MAXLAT,MINLON),(MINLAT,MINLON)]\n",
    "\n",
    "import folium\n",
    "m = folium.Map(location=[37.718890744427554, -118.88748662532234], zoom_start=8, tiles=\"Stamen Terrain\")\n",
    "folium.Marker(\n",
    "    [37.718890744427554, -118.88748662532234], popup=\"<b>Long Valley Caldera</b>\").add_to(m)\n",
    "folium.Polygon(bbox_poly).add_to(m)\n",
    "\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Download Orbit Files based on SLCs ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "get_orbit_files()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Download and stitch DEM ####\n",
    "\n",
    "Download of DEM (need to use wgs84 version) using the ISCE DEM download script.\n",
    "\n",
    "```\n",
    "mkdir DEM; cd DEM\n",
    "dem.py -a stitch -b 18 20 -100 -97 -r -s 1 –c\n",
    "rm demLat*.dem demLat*.dem.xml demLat*.dem.vrt\n",
    "cd ..\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "download_dem()\n",
    "\n",
    "cwd = os.getcwd()\n",
    "print(\"cwd : {}\".format(cwd))\n",
    "if os.path.exists(\"dem.txt\"):\n",
    "    cmd = [\"awk\", \"'/wgs84/ {print $NF;exit}'\", \"dem.txt\"]\n",
    "    WGS84 = run_cmd_output(cmd).decode(\"utf-8\").strip()\n",
    "    wgs84_file = os.path.join(cwd, WGS84)\n",
    "    print(\"WGS84 : a{}b\".format(wgs84_file))\n",
    "    if os.path.exists(wgs84_file):\n",
    "        print(\"Found wgs84 file: {}\".format(wgs84_file))\n",
    "        fix_cmd = [\"{}/applications/fixImageXml.py\".format(ISCE_HOME), \"--full\", \"-i\", \"{}\".format(wgs84_file) ]\n",
    "        run_cmd(fix_cmd) \n",
    "    else:\n",
    "        print(\"NO WGS84 FILE FOUND : {}\".format(wgs84_file))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### AUX_CAL file download ####\n",
    "\n",
    "The following calibration auxliary (AUX_CAL) file is used for **antenna pattern correction** to compensate the range phase offset of SAFE products with **IPF verison 002.36** (mainly for images acquired before March 2015). If all your SAFE products are from another IPF version, then no AUX files are needed. Check [ESA document](https://earth.esa.int/documents/247904/1653440/Sentinel-1-IPF_EAP_Phase_correction) for details. \n",
    "\n",
    "Make sure your aws credentials are fresh (i.e. run 'aws-login -p default' in a terminal window) and run the command below to download the AUX_CAL file once and store it somewhere (_i.e._ ~/aux/aux_cal) so that you can use it all the time, for `stackSentinel.py -a` or `auxiliary data directory` in `topsApp.py`.\n",
    "\n",
    "```aws s3 cp --recursive s3://nisar-dev-ondemand/S1_aux/  ~/aux/aux_cal/```\n",
    "\n",
    "In the cell below, the necessary AUX_CAL file is copied to the AuxDir directory from this download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "mkdir -p ./AuxDir\n",
    "cp ~/aux/aux_cal/S1A/S1A_AUX_CAL_V20140915T100000_G20151125T103928.SAFE/data/s1a-aux-cal.xml ./AuxDir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## STACK SENTINEL DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "The scripts provides support for Sentinel-1 TOPS stack processing. Currently supported workflows include a coregistered stack of SLC, interferograms, offsets, and coherence. \n",
    "\n",
    "`stackSentinel.py` generates all configuration and run files required to be executed on a stack of Sentinel-1 TOPS data. When stackSentinel.py is executed for a given workflow (-W option) a **configs** and **run_files** folder is generated. No processing is performed at this stage. Within the run_files folder different run\\_#\\_description files are contained which are to be executed as shell scripts in the run number order. Each of these run scripts call specific configure files contained in the “configs” folder which call ISCE in a modular fashion. The configure and run files will change depending on the selected workflow. To make run_# files executable, change the file permission accordingly (e.g., `chmod +x run_01_unpack_slc`).\n",
    "\n",
    "```bash\n",
    "stackSentinel.py -H     #To see workflow examples,\n",
    "stackSentinel.py -h     #To get an overview of all the configurable parameters\n",
    "```\n",
    "\n",
    "Required parameters of stackSentinel.py include:\n",
    "\n",
    "```cfg\n",
    "-s SLC_DIRNAME          #A folder with downloaded Sentinel-1 SLC’s. \n",
    "-o ORBIT_DIRNAME        #A folder containing the Sentinel-1 orbits. Missing orbit files will be downloaded automatically\n",
    "-a AUX_DIRNAME          #A folder containing the Sentinel-1 Auxiliary files\n",
    "-d DEM_FILENAME         #A DEM (Digital Elevation Model) referenced to wgs84\n",
    "```\n",
    "\n",
    "In the following, different workflow examples are provided. Note that stackSentinel.py only generates the run and configure files. To perform the actual processing, the user will need to execute each run file in their numbered order.\n",
    "\n",
    "In all workflows, coregistration (-C option) can be done using only geometry (set option = geometry) or with geometry plus refined azimuth offsets through NESD (set option = NESD) approach, the latter being the default. For the NESD coregistrstion the user can control the ESD coherence threshold (-e option) and the number of overlap interferograms (-O) to be used in NESD estimation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "master_date=get_master_date(ctx)\n",
    "os.chdir(wd)\n",
    "print(\"master_date : {}\".format(master_date))\n",
    "\n",
    "cmd = [\n",
    "    \"stackSentinel.py\", \"-s\", \"zip/\", \"-d\", \"{}\".format(wgs84_file), \"-a\", \"AuxDir/\", \"-m\", \"{}\".format(master_date), \"-o\", \"orbits\", \n",
    "    \"-b\", \"\\\"{} {} {} {}\\\"\".format(MINLAT, MAXLAT, MINLON, MAXLON), \n",
    "    \"-W\", \"interferogram\", \"-C\", \"geometry\"\n",
    "]\n",
    "run_cmd(cmd)          \n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "cmd = [\"topsstack-hamsar/topsStack/stackSlcDn_run2.5.sh\", \"{} {} {} {}\".format(MINLAT, MAXLAT, MINLON, MAXLON)]\n",
    "run_cmd(cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Run all steps ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "!chmod +x run_files/*\n",
    "\n",
    "# Step 1\n",
    "!run_files/run_01_unpack_topo_reference > run1.log;\n",
    "print('Step 1 Finished!')\n",
    "print()\n",
    "# Step 2\n",
    "!cat run_files/run_02_unpack_secondary_slc | parallel -j2 --eta --load 50% > run2.log;\n",
    "print('Step 2 Finished!')\n",
    "print()\n",
    "# Step 2.5\n",
    "!cat run_files/run_02.5_slc_noise_calibration | parallel -j2 --eta --load 50% > run2_5.log;\n",
    "print('Step 2.5 Finished!')\n",
    "print()\n",
    "# Step 3\n",
    "!cat run_files/run_03_average_baseline | parallel -j2 --eta --load 50% > run3.log;\n",
    "print('Step 3 Finished!')\n",
    "print()\n",
    "# Step 4\n",
    "!cat run_files/run_04_fullBurst_geo2rdr  | parallel -j2 --eta --load 50% > run4.log;\n",
    "print('Step 4 Finished!')\n",
    "print()\n",
    "# Step 5\n",
    "!cat run_files/run_05_fullBurst_resample  | parallel -j2 --eta --load 50% > run5.log;\n",
    "print('Step 5 Finished!')\n",
    "print()\n",
    "# Step 6\n",
    "!sh run_files/run_06_extract_stack_valid_region > run6.log;\n",
    "print('Step 6 Finished!')\n",
    "print()\n",
    "# Step 7\n",
    "!cat run_files/run_07_merge_reference_secondary_slc | parallel -j2 --eta --load 50% > run7.log;\n",
    "print('Step 7 Finished!')\n",
    "print()\n",
    "# Step 8\n",
    "!cat run_files/run_08_generate_burst_igram | parallel -j2 --eta --load 50% > run8.log;\n",
    "print('Step 8 Finished!')\n",
    "print()\n",
    "# Step 9\n",
    "!cat run_files/run_09_merge_burst_igram | parallel -j2 --eta --load 50% > run9.log;\n",
    "print('Step 9 Finished!')\n",
    "print()\n",
    "# Step 10\n",
    "!cat run_files/run_10_filter_coherence | parallel -j2 --eta --load 50% > run10.log;\n",
    "print('Step 10 Finished!')\n",
    "print()\n",
    "# Step 11\n",
    "!cat run_files/run_11_unwrap | parallel -j2 --eta --load 50% > run11.log;\n",
    "print('Step 11 Finished!')\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Display output products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ifgdir = os.path.join(wd,'merged','interferograms')\n",
    "ifglist = os.listdir(ifgdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Go through interferogram folder and plot things.\n",
    "ifgdir = os.path.join(wd,'merged','interferograms')\n",
    "ifglist = os.listdir(ifgdir)\n",
    "\n",
    "for i in ifglist:\n",
    "    \n",
    "    # Wrapped Interferograms\n",
    "    wifg = os.path.join(ifgdir,i,'filt_fine.int')\n",
    "    ds = gdal.Open(wifg)\n",
    "    arr = ds.GetRasterBand(1).ReadAsArray()\n",
    "    ph = np.angle(arr)\n",
    "    arrx = arr.shape[1]\n",
    "    arry = arr.shape[0]\n",
    "    xx = np.arange(arrx)\n",
    "    yy = np.arange(arry)\n",
    "    \n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.pcolormesh(xx, yy, ph,shading='nearest',cmap='jet');\n",
    "    plt.title(i + ' - wrapped')\n",
    "    \n",
    "    outfig = os.path.join(ifgdir,i, i + '_wrap_ra.png')\n",
    "    plt.savefig(outfig)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "    # Unwrapped Interferograms\n",
    "    uifg = os.path.join(ifgdir,i,'filt_fine.unw')\n",
    "    ds = gdal.Open(uifg)\n",
    "    arr = ds.GetRasterBand(2).ReadAsArray()\n",
    "    arrx = arr.shape[1]\n",
    "    arry = arr.shape[0]\n",
    "    xx = np.arange(arrx)\n",
    "    yy = np.arange(arry)\n",
    "    \n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.pcolormesh(xx, yy, arr,shading='nearest',cmap='jet');\n",
    "    plt.title(i + ' - unwrapped')\n",
    "    \n",
    "    outfig = os.path.join(ifgdir,i,i + '_unw_ra.png')\n",
    "    plt.savefig(outfig)\n",
    "    plt.close()\n",
    "\n",
    "    # Correlation\n",
    "    wcorr = os.path.join(ifgdir,i,'filt_fine.cor')\n",
    "    ds = gdal.Open(wcorr)\n",
    "    arr = ds.GetRasterBand(1).ReadAsArray()\n",
    "    arrx = arr.shape[1]\n",
    "    arry = arr.shape[0]\n",
    "    xx = np.arange(arrx)\n",
    "    yy = np.arange(arry)\n",
    "    \n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.pcolormesh(xx, yy, arr,shading='nearest',cmap='gray');\n",
    "    plt.title(i + ' - correlation')\n",
    "    \n",
    "    outfig = os.path.join(ifgdir,i,i + '_cor_ra.png')\n",
    "    plt.savefig(outfig)\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "ifgdir = os.path.join(wd,'merged','interferograms')\n",
    "ifglist = os.listdir(ifgdir)\n",
    "ifglist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Display wrapped interferograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "from ipywidgets import interact\n",
    "\n",
    "ifg_ra = []\n",
    "for i in ifglist:\n",
    "    for f in os.listdir(os.path.join(ifgdir,i)):\n",
    "        #print(f)\n",
    "        if 'png' in f and 'wrap' in f:\n",
    "            ifg_ra.append(os.path.join(ifgdir,i,f))\n",
    "@interact\n",
    "def show_images(file=ifg_ra):\n",
    "    display(Image(file))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Display unwrapped interferograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "ifg_ra = []\n",
    "for i in ifglist:\n",
    "    for f in os.listdir(os.path.join(ifgdir,i)):\n",
    "        #print(f)\n",
    "        if 'png' in f and 'unw' in f:\n",
    "            ifg_ra.append(os.path.join(ifgdir,i,f))\n",
    "@interact\n",
    "def show_images(file=ifg_ra):\n",
    "    display(Image(file))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display correlation plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ifg_ra = []\n",
    "for i in ifglist:\n",
    "    for f in os.listdir(os.path.join(ifgdir,i)):\n",
    "        #print(f)\n",
    "        if 'png' in f and 'cor' in f:\n",
    "            ifg_ra.append(os.path.join(ifgdir,i,f))\n",
    "@interact\n",
    "def show_images(file=ifg_ra):\n",
    "    display(Image(file))\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Interferograms\n",
    "for i in ifglist:\n",
    "    for f in os.listdir(os.path.join(wd,'merged','interferograms',i)):\n",
    "        if f.split('.')[-1]=='unw':\n",
    "            os.chdir(os.path.join(wd,'merged','interferograms',i))\n",
    "            comm = 'geocodeGdal.py  -l ' + os.path.join(wd,'merged','geom_reference','lat.rdr') \\\n",
    "            + ' -L ' + os.path.join(wd,'merged','geom_reference','lon.rdr') \\\n",
    "            + ' -f '+ os.path.join(wd,'merged','interferograms',i,f) \\\n",
    "            + ' -b \"' + str(MINLAT) + ' ' + str(MAXLAT) + ' ' + str(MINLON) + ' ' + str(MAXLON) + '\"'\n",
    "            print(comm)\n",
    "            os.system(comm)\n",
    "\n",
    "os.chdir(wd)       "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
