{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"1\">Copyright 2021, by the California Institute of Technology. ALL RIGHTS RESERVED. United States Government sponsorship acknowledged. Any commercial use must be negotiated with the Office of Technology Transfer at the California Institute of Technology.</font>\n",
    "    \n",
    "<font size=\"1\">This software may be subject to U.S. export control laws and regulations. By accepting this document, the user agrees to comply with all applicable U.S. export laws and regulations. User has the responsibility to obtain export licenses, or other export authority as may be required, before exporting such information to foreign countries or providing access to foreign persons.<font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing TopsApp\n",
    "\n",
    "## Introduction\n",
    "In this notebook, we will run the various steps of processing with topsApp.py. \n",
    "\n",
    "topsApp.py is a pair-by-pair interferometric processor that takes as input two Sentinel-1 SAR acquisitions acquired in TOPS mode.At this time, topsApp only supports SLC data from Sentinel-1 A and B. Processing is supported across the Sentinel-1 constellation, i.e. data acquired from A and B can be combined**\n",
    "\n",
    "Processing of TopsApp involves:\n",
    "\n",
    "**Downloading Inputs**\n",
    "  - Downloading SLCs : Both Reference and Secondary\n",
    "  - Downlaoding DEMs : Based on supplied region of interest (min/max lat/lon)\n",
    "  - Downloading Orbits : Based on SLC dates\n",
    "\n",
    "**Processing with ISCE**\n",
    "  - Creating ISCE input configuration files : topsApp.xml, reference.xml, secondary.xml\n",
    "  - Steps of topsApp processing (in order):\n",
    "     - startup\n",
    "     - preprocess\n",
    "     - computeBaselines**\n",
    "     - verifyDEM\n",
    "     - topo\n",
    "     - subsetoverlaps\n",
    "     - coarseoffsets\n",
    "     - coarseresamp\n",
    "     - overlapifg\n",
    "     - prepesd\n",
    "     - esd\n",
    "     - rangecoreg\n",
    "     - fineoffsets\n",
    "     - fineresamp\n",
    "     - ion\n",
    "     - burstifg\n",
    "     - mergebursts\n",
    "     - filter\n",
    "     - unwrap\n",
    "     - unwrap2stage\n",
    "     - geocode\n",
    "     - denseoffsets\n",
    "     - filteroffsets\n",
    "     - geocodeoffsets\n",
    "     \n",
    "     The steps of topsApp depends on start and end parameter supplied while calling topsApp. For example, the following command will process from the first step (startup) till step 'geocode'\n",
    "    ```\n",
    "     /opt/isce2/isce/applications/topsApp.py --start=startup --end=geocode\n",
    "    ```\n",
    "    \n",
    "    Also, some steps can be turned on or off by setting some properties in topsApp.xml. For example, 'unwrap' step will be ignored if'do_unwrap' property value is set to False\n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configurable Parameters\n",
    "\n",
    "We can run this notebook to process topsApp with many different combinations of SLCs and region of interest as well as with different combination of topsApp properties like swaths, azimuth looks, range looks etc. In the followinng section we initizale all these tunable variables.\n",
    "\n",
    "**Input Parameters**\n",
    "\n",
    "- **min_lat, max_lat, min_loc, max_loc**: min/max latitude and longditude of Region of Interest.\n",
    "    Example:\n",
    "        min_lat = 31.9 # type: number\n",
    "- **reference_slcs, secondary_slcs** : List of reference SLCs and secondary SLCs.\n",
    "    Example:\n",
    "        reference_slcs = [\"S1B_IW_SLC__1SDV_20190628T014909_20190628T014936_016890_01FC87_55C8\"]\n",
    "        \n",
    "**Tunable Parameters**\n",
    "- **swaths** : array conntaining swath values to be considerate.\n",
    "    Example:\n",
    "        swaths = [3]\n",
    "- **range_looks** : range looks value. Number.\n",
    "    Example: \n",
    "        range_looks = 7        \n",
    "- **azimuth_looks** : azimuth looks value. Number.\n",
    "    Example:\n",
    "        azimuth_looks = 3\n",
    "- **do_unwrap** : True or False if unwrapping shoud be done or not.\n",
    "    Example:\n",
    "        do_unwrap = \"True\"\n",
    "- **unwrapper_name** : Unwrapper name when do_unwrap is True.\n",
    "    Example:\n",
    "        unwrapper_name = \"snaphu_mcf\"\n",
    "- **do_denseoffsets** : True/False for denseoffsets processing.\n",
    "    Example:\n",
    "        do_denseoffsets = \"False\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "from typing import List\n",
    "\n",
    "min_lat = 31.9 # type: number\n",
    "max_lat = 33.94 # type: number\n",
    "min_lon =  -118.74 # type: number\n",
    "max_lon = -115.69 # type: number\n",
    "        \n",
    "reference_slcs: List[str] = [\"S1B_IW_SLC__1SDV_20190628T014909_20190628T014936_016890_01FC87_55C8\"]\n",
    "secondary_slcs: List[str] = [\"S1B_IW_SLC__1SDV_20190710T014909_20190710T014936_017065_0201B8_0252\"]                     \n",
    "\n",
    "    \n",
    "sensor_name = \"SENTINEL1\"\n",
    "swaths: List[int] = [1, 2, 3]\n",
    "range_looks = 7\n",
    "azimuth_looks = 3\n",
    "do_unwrap = \"False\"\n",
    "unwrapper_name = \"snaphu_mcf\"\n",
    "do_denseoffsets = \"False\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility Functions : Plotting Functions\n",
    "\n",
    "We use these functions to plot different figures to visualize the outputs. Matplotlib is used to generate the plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shutil import copyfile, move # Utilities for copying and moving files\n",
    "from osgeo import gdal            # GDAL support for reading virtual files\n",
    "import os                         # To create and remove directories\n",
    "import matplotlib.pyplot as plt   # For plotting\n",
    "import numpy as np                # Matrix calculations\n",
    "import glob                       # Retrieving list of files\n",
    "import boto3                      # For talking to s3 bucket\n",
    "import rasterio as rio\n",
    "from rasterio.plot import show, plotting_extent\n",
    "from rasterio.merge import merge\n",
    "\n",
    "# directory in which the notebook resides\n",
    "if 'tutorial_home_dir' not in globals():\n",
    "    tutorial_home_dir = os.getcwd()\n",
    "print(\"Notebook directory: \", tutorial_home_dir)\n",
    "# assumption is this is in the notebook_pges subdirectory of the repo\n",
    "base_dir = os.path.abspath(os.path.join(tutorial_home_dir, '..'))\n",
    "\n",
    "# directory for data downloads\n",
    "output_dir = os.path.join(base_dir, 'notebook_output', 'topsApp')\n",
    "slc_dir = os.path.join(output_dir, 'data', 'slcs')\n",
    "orbit_dir = os.path.join(output_dir, 'data', 'orbits')\n",
    "insar_dir = os.path.join(output_dir, 'insar')\n",
    "\n",
    "# defining backup dirs in case of download issues on the local server\n",
    "s3 = boto3.resource(\"s3\")\n",
    "data_backup_bucket = s3.Bucket(\"asf-jupyter-data\")\n",
    "data_backup_dir = \"TOPS\"\n",
    "\n",
    "# generate all the folders in case they do not exist yet\n",
    "os.makedirs(slc_dir, exist_ok=True)\n",
    "os.makedirs(orbit_dir, exist_ok=True)\n",
    "os.makedirs(insar_dir, exist_ok=True)\n",
    "\n",
    "# Always start at the notebook directory    \n",
    "os.chdir(tutorial_home_dir)\n",
    "\n",
    "#Plot data using folio\n",
    "def plot_wrapped_data_multiframe(frame_list):\n",
    "    import os\n",
    "    import folium\n",
    "    from glob import glob\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    import rasterio as rio\n",
    "    from rasterio.plot import show, plotting_extent\n",
    "    from rasterio.merge import merge\n",
    "    from PIL import Image, ImageChops\n",
    "\n",
    "\n",
    "    # plot wrapped IFGs individually\n",
    "    flat_plots = []\n",
    "    flat_bboxes = []\n",
    "    for i, file in enumerate(frame_list):\n",
    "        src = rio.open(file)\n",
    "        fig, ax = plt.subplots(1, figsize=(18, 16))\n",
    "        data = src.read(1)\n",
    "        data[data==0] = np.nan\n",
    "        show(np.angle(data), cmap='rainbow', vmin=-np.pi, vmax=np.pi, transform=src.transform, ax=ax)\n",
    "        png_file = f'flat_{i}.png'\n",
    "        fig.savefig(png_file, transparent=True)\n",
    "        flat_plots.append(png_file)\n",
    "        flat_bboxes.append(src.bounds)\n",
    "\n",
    "def plot_wrapped_data_singleframe(filename='merged/filt_topophase.flat.geo'):\n",
    "    import os\n",
    "    import folium\n",
    "    from glob import glob\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    import rasterio as rio\n",
    "    from rasterio.plot import show, plotting_extent\n",
    "    from rasterio.merge import merge\n",
    "    from PIL import Image, ImageChops\n",
    "\n",
    "\n",
    "    src = rio.open(filename)\n",
    "\n",
    "    fig, ax = plt.subplots(1, figsize=(18, 16))\n",
    "    data = src.read(1)\n",
    "    data[data==0] = np.nan\n",
    "    show(np.angle(data), cmap='rainbow', vmin=-np.pi, vmax=np.pi, transform=src.transform, ax=ax)\n",
    "    png_file = f'flat.png'\n",
    "    fig.savefig(png_file, transparent=True)\n",
    "    \n",
    "# Utility to plot a 2D array\n",
    "def plotdata(GDALfilename, band=1,\n",
    "             title=None,colormap='gray',\n",
    "             aspect=1, background=None,\n",
    "             datamin=None, datamax=None,\n",
    "             interpolation='nearest',\n",
    "             nodata = None,\n",
    "             draw_colorbar=True, colorbar_orientation=\"horizontal\"):\n",
    "    \n",
    "    # Read the data into an array\n",
    "    ds = gdal.Open(GDALfilename, gdal.GA_ReadOnly)\n",
    "    data = ds.GetRasterBand(band).ReadAsArray()\n",
    "    transform = ds.GetGeoTransform()\n",
    "    ds = None\n",
    "    \n",
    "    try:\n",
    "        if nodata is not None:\n",
    "            data[data == nodata] = np.nan\n",
    "    except:\n",
    "        pass\n",
    "        \n",
    "    # getting the min max of the axes\n",
    "    firstx = transform[0]\n",
    "    firsty = transform[3]\n",
    "    deltay = transform[5]\n",
    "    deltax = transform[1]\n",
    "    lastx = firstx+data.shape[1]*deltax\n",
    "    lasty = firsty+data.shape[0]*deltay\n",
    "    ymin = np.min([lasty,firsty])\n",
    "    ymax = np.max([lasty,firsty])\n",
    "    xmin = np.min([lastx,firstx])\n",
    "    xmax = np.max([lastx,firstx])\n",
    "\n",
    "    # put all zero values to nan and do not plot nan\n",
    "    if background is None:\n",
    "        try:\n",
    "            data[data==0]=np.nan\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    fig = plt.figure(figsize=(18, 16))\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.imshow(data, vmin = datamin, vmax=datamax,\n",
    "                    cmap=colormap, extent=[xmin,xmax,ymin,ymax],\n",
    "                    interpolation=interpolation)\n",
    "    ax.set_title(title)\n",
    "    if draw_colorbar is not None:\n",
    "        cbar = fig.colorbar(cax,orientation=colorbar_orientation)\n",
    "    ax.set_aspect(aspect)    \n",
    "    plt.show()\n",
    "    \n",
    "    # clearing the data\n",
    "    data = None\n",
    "\n",
    "def plot_wrapped_multifiles(files, figsize=(20, 30)):\n",
    "    files_to_mosaic = []\n",
    "    for file in files: #glob(\"hello_world*/filt_topophase.flat.geo\"):\n",
    "        src = rio.open(file)\n",
    "        files_to_mosaic.append(src)\n",
    "    \n",
    "    mosaic, out_trans = merge(files_to_mosaic, nodata=0)\n",
    "\n",
    "    mosaic[mosaic==0] = np.nan\n",
    "    fig, ax = plt.subplots(1, figsize=figsize)\n",
    "    show(np.angle(mosaic[0]), cmap='rainbow', vmin=-np.pi, vmax=np.pi, ax=ax)\n",
    "\n",
    "def plot_unwrapped_multifiles(files, figsize=(20, 30)):\n",
    "    files_to_mosaic = []\n",
    "    for file in files:\n",
    "        src = rio.open(file)\n",
    "        files_to_mosaic.append(src)\n",
    "    \n",
    "    mosaic, out_trans = merge(files_to_mosaic, nodata=0)\n",
    "\n",
    "    mosaic[mosaic==0] = np.nan\n",
    "    fig, ax = plt.subplots(1, figsize=figsize)\n",
    "    show(mosaic[1], cmap='jet', vmin=-20, vmax=50, ax=ax)\n",
    "    \n",
    "# Utility to plot interferograms\n",
    "def plotcomplexdata(GDALfilename,\n",
    "                    title=None, aspect=1,\n",
    "                    datamin=None, datamax=None,\n",
    "                    interpolation='nearest',\n",
    "                    draw_colorbar=None, colorbar_orientation=\"horizontal\"):\n",
    "    # Load the data into numpy array\n",
    "    ds = gdal.Open(GDALfilename, gdal.GA_ReadOnly)\n",
    "    slc = ds.GetRasterBand(1).ReadAsArray()\n",
    "    transform = ds.GetGeoTransform()\n",
    "    ds = None\n",
    "    \n",
    "    # getting the min max of the axes\n",
    "    firstx = transform[0]\n",
    "    firsty = transform[3]\n",
    "    deltay = transform[5]\n",
    "    deltax = transform[1]\n",
    "    lastx = firstx+slc.shape[1]*deltax\n",
    "    lasty = firsty+slc.shape[0]*deltay\n",
    "    ymin = np.min([lasty,firsty])\n",
    "    ymax = np.max([lasty,firsty])\n",
    "    xmin = np.min([lastx,firstx])\n",
    "    xmax = np.max([lastx,firstx])\n",
    "\n",
    "    # put all zero values to nan and do not plot nan\n",
    "    try:\n",
    "        slc[slc==0]=np.nan\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    \n",
    "    fig = plt.figure(figsize=(18, 16))\n",
    "    ax = fig.add_subplot(1,2,1)\n",
    "    cax1=ax.imshow(np.abs(slc), vmin = datamin, vmax=datamax,\n",
    "                   cmap='gray', extent=[xmin,xmax,ymin,ymax],\n",
    "                   interpolation=interpolation)\n",
    "    ax.set_title(title + \" (amplitude)\")\n",
    "    if draw_colorbar is not None:\n",
    "        cbar1 = fig.colorbar(cax1,orientation=colorbar_orientation)\n",
    "    ax.set_aspect(aspect)\n",
    "\n",
    "    ax = fig.add_subplot(1,2,2)\n",
    "    cax2 =ax.imshow(np.angle(slc), cmap='rainbow',\n",
    "                    vmin=-np.pi, vmax=np.pi,\n",
    "                    extent=[xmin,xmax,ymin,ymax],\n",
    "                    interpolation=interpolation)\n",
    "    ax.set_title(title + \" (phase [rad])\")\n",
    "    if draw_colorbar is not None:\n",
    "        cbar2 = fig.colorbar(cax2, orientation=colorbar_orientation)\n",
    "    ax.set_aspect(aspect)\n",
    "    plt.show()\n",
    "    \n",
    "    # clearing the data\n",
    "    slc = None\n",
    "\n",
    "# Utility to plot multiple similar arrays\n",
    "def plotstackdata(GDALfilename_wildcard, band=1,\n",
    "                  title=None, colormap='gray',\n",
    "                  aspect=1, datamin=None, datamax=None,\n",
    "                  interpolation='nearest',\n",
    "                  draw_colorbar=True, colorbar_orientation=\"horizontal\"):\n",
    "    # get a list of all files matching the filename wildcard criteria\n",
    "    GDALfilenames = glob.glob(GDALfilename_wildcard)\n",
    "    \n",
    "    # initialize empty numpy array\n",
    "    data = None\n",
    "    for GDALfilename in GDALfilenames:\n",
    "        ds = gdal.Open(GDALfilename, gdal.GA_ReadOnly)\n",
    "        data_temp = ds.GetRasterBand(band).ReadAsArray()   \n",
    "        ds = None\n",
    "        \n",
    "        if data is None:\n",
    "            data = data_temp\n",
    "        else:\n",
    "            data = np.vstack((data,data_temp))\n",
    "\n",
    "    # put all zero values to nan and do not plot nan\n",
    "    try:\n",
    "        data[data==0]=np.nan\n",
    "    except:\n",
    "        pass            \n",
    "            \n",
    "    fig = plt.figure(figsize=(18, 16))\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.imshow(data, vmin = datamin, vmax=datamax,\n",
    "                    cmap=colormap, interpolation=interpolation)\n",
    "    ax.set_title(title)\n",
    "    if draw_colorbar is not None:\n",
    "        cbar = fig.colorbar(cax,orientation=colorbar_orientation)\n",
    "    ax.set_aspect(aspect)    \n",
    "    plt.show() \n",
    "\n",
    "    # clearing the data\n",
    "    data = None\n",
    "\n",
    "# Utility to plot multiple simple complex arrays\n",
    "def plotstackcomplexdata(GDALfilename_wildcard,\n",
    "                         title=None, aspect=1,\n",
    "                         datamin=None, datamax=None,\n",
    "                         interpolation='nearest',\n",
    "                         draw_colorbar=True, colorbar_orientation=\"horizontal\"):\n",
    "    # get a list of all files matching the filename wildcard criteria\n",
    "    GDALfilenames = glob.glob(GDALfilename_wildcard)\n",
    "    print(GDALfilenames)\n",
    "    # initialize empty numpy array\n",
    "    data = None\n",
    "    for GDALfilename in GDALfilenames:\n",
    "        ds = gdal.Open(GDALfilename, gdal.GA_ReadOnly)\n",
    "        data_temp = ds.GetRasterBand(1).ReadAsArray()\n",
    "        ds = None\n",
    "        \n",
    "        if data is None:\n",
    "            data = data_temp\n",
    "        else:\n",
    "            data = np.vstack((data,data_temp))\n",
    "\n",
    "    # put all zero values to nan and do not plot nan\n",
    "    try:\n",
    "        data[data==0]=np.nan\n",
    "    except:\n",
    "        pass              \n",
    "            \n",
    "    fig = plt.figure(figsize=(18, 16))\n",
    "    ax = fig.add_subplot(1,2,1)\n",
    "    cax1=ax.imshow(np.abs(data), vmin=datamin, vmax=datamax,\n",
    "                   cmap='gray', interpolation='nearest')\n",
    "    ax.set_title(title + \" (amplitude)\")\n",
    "    if draw_colorbar is not None:\n",
    "        cbar1 = fig.colorbar(cax1,orientation=colorbar_orientation)\n",
    "    ax.set_aspect(aspect)\n",
    "\n",
    "    ax = fig.add_subplot(1,2,2)\n",
    "    cax2 =ax.imshow(np.angle(data), cmap='rainbow',\n",
    "                            interpolation='nearest')\n",
    "    ax.set_title(title + \" (phase [rad])\")\n",
    "    if draw_colorbar is not None:\n",
    "        cbar2 = fig.colorbar(cax2,orientation=colorbar_orientation)\n",
    "    ax.set_aspect(aspect)\n",
    "    plt.show() \n",
    "    \n",
    "    # clearing the data\n",
    "    data = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Utility Functions \n",
    "Functions related to perform downloading of inputs, such as SLCs, dems, orbits etc and generating the config files dynamically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from math import floor, ceil\n",
    "import json\n",
    "import re\n",
    "import osaka\n",
    "import osaka.main\n",
    "from builtins import str\n",
    "import os, sys, re, json, logging, traceback, requests, argparse\n",
    "from datetime import datetime\n",
    "from pprint import pformat\n",
    "from requests.packages.urllib3.exceptions import (InsecureRequestWarning,\n",
    "                                                  InsecurePlatformWarning)\n",
    "import isce\n",
    "from iscesys.Component.ProductManager import ProductManager as PM\n",
    "\n",
    "try: from html.parser import HTMLParser\n",
    "except: from html.parser import HTMLParser\n",
    "    \n",
    "log_format = \"[%(asctime)s: %(levelname)s/%(funcName)s] %(message)s\"\n",
    "logging.basicConfig(format=log_format, level=logging.INFO)\n",
    "logger = logging.getLogger('create_ifg')\n",
    "\n",
    "        \n",
    "requests.packages.urllib3.disable_warnings(InsecureRequestWarning)\n",
    "requests.packages.urllib3.disable_warnings(InsecurePlatformWarning)\n",
    "PROCESSING_START=datetime.now().strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
    "\n",
    "PGE_BASE=os.getcwd()\n",
    "ISCE_HOME=\"/opt/isce2/isce\"\n",
    "\n",
    "SLC_RE = re.compile(r'(?P<mission>S1\\w)_IW_SLC__.*?' +\n",
    "                    r'_(?P<start_year>\\d{4})(?P<start_month>\\d{2})(?P<start_day>\\d{2})' +\n",
    "                    r'T(?P<start_hour>\\d{2})(?P<start_min>\\d{2})(?P<start_sec>\\d{2})' +\n",
    "                    r'_(?P<end_year>\\d{4})(?P<end_month>\\d{2})(?P<end_day>\\d{2})' +\n",
    "                    r'T(?P<end_hour>\\d{2})(?P<end_min>\\d{2})(?P<end_sec>\\d{2})_.*$')\n",
    "\n",
    "\n",
    "QC_SERVER = 'https://qc.sentinel1.eo.esa.int/'\n",
    "DATA_SERVER = 'http://aux.sentinel1.eo.esa.int/'\n",
    "\n",
    "ORBITMAP = [('precise','aux_poeorb', 100),\n",
    "            ('restituted','aux_resorb', 100)]\n",
    "\n",
    "OPER_RE = re.compile(r'S1\\w_OPER_AUX_(?P<type>\\w+)_OPOD_(?P<yr>\\d{4})(?P<mo>\\d{2})(?P<dy>\\d{2})')\n",
    "\n",
    "wd = os.getcwd()\n",
    "\n",
    "if isinstance(reference_slcs, str):\n",
    "    reference_slcs = json.loads(reference_slcs)\n",
    "if isinstance(secondary_slcs, str):\n",
    "    secondary_slcs = json.loads(secondary_slcs)\n",
    "    \n",
    "if isinstance(swaths, str):\n",
    "    swaths = json.loads(swaths)\n",
    "    \n",
    "localize_slcs = reference_slcs + secondary_slcs\n",
    "tops_properties = {}\n",
    "tops_properties[\"swaths\"] = swaths\n",
    "tops_properties[\"range looks\"] = range_looks\n",
    "tops_properties[\"azimuth looks\"] = azimuth_looks\n",
    "tops_properties[\"do unwrap\"] = do_unwrap\n",
    "tops_properties[\"unwrapper name\"] = unwrapper_name\n",
    "tops_properties[\"do denseoffsets\"] = do_denseoffsets\n",
    "tops_properties[\"region of interest\"] =\"[{}, {}, {}, {}]\".format(min_lat, max_lat, min_lon, max_lon)\n",
    "wgs84_file = ''\n",
    "\n",
    "class MyHTMLParser(HTMLParser):\n",
    "\n",
    "    def __init__(self):\n",
    "        HTMLParser.__init__(self)\n",
    "        self.fileList = []\n",
    "        self.pages = 0\n",
    "        self.in_td = False\n",
    "        self.in_a = False\n",
    "        self.in_ul = False\n",
    "\n",
    "    def handle_starttag(self, tag, attrs):\n",
    "        if tag == 'td':\n",
    "            self.in_td = True\n",
    "        elif tag == 'a' and self.in_td:\n",
    "            self.in_a = True\n",
    "        elif tag == 'ul':\n",
    "            for k,v in attrs:\n",
    "                if k == 'class' and v.startswith('pagination'):\n",
    "                    self.in_ul = True\n",
    "        elif tag == 'li' and self.in_ul:\n",
    "            self.pages += 1\n",
    "\n",
    "    def handle_data(self,data):\n",
    "        if self.in_td and self.in_a:\n",
    "            if OPER_RE.search(data):\n",
    "                self.fileList.append(data.strip())\n",
    "\n",
    "    def handle_endtag(self, tag):\n",
    "        if tag == 'td':\n",
    "            self.in_td = False\n",
    "            self.in_a = False\n",
    "        elif tag == 'a' and self.in_td:\n",
    "            self.in_a = False\n",
    "        elif tag == 'ul' and self.in_ul:\n",
    "            self.in_ul = False\n",
    "        elif tag == 'html':\n",
    "            if self.pages == 0:\n",
    "                self.pages = 1\n",
    "            else:\n",
    "                # decrement page back and page forward list items\n",
    "                self.pages -= 2\n",
    "\n",
    "def session_get(session, url):\n",
    "    return session.get(url, verify=False)\n",
    "\n",
    "def get_download_orbit_dict(download_orbit_dict, slc_date, mission_type):\n",
    "    \n",
    "\n",
    "    logger.info(\"slc_date : {}\".format(slc_date))\n",
    "    url = \"https://qc.sentinel1.eo.esa.int/aux_poeorb/?validity_start={}&sentinel1__mission={}\".format(slc_date, mission_type)\n",
    "    session = requests.Session()\n",
    "    r = session_get(session, url)\n",
    "    r.raise_for_status()\n",
    "    parser = MyHTMLParser()\n",
    "    parser.feed(r.text)\n",
    "\n",
    "    for res in parser.fileList:\n",
    "        #id = \"%s-%s\" % (os.path.splitext(res)[0], dataset_version)\n",
    "        match = OPER_RE.search(res)\n",
    "        if not match:\n",
    "            raise RuntimeError(\"Failed to parse orbit: {}\".format(res))\n",
    "        download_orbit_dict[res] = os.path.join(DATA_SERVER, \"/\".join(match.groups()), \"{}.EOF\".format(res))\n",
    "        #yield id, results[id]\n",
    "        \n",
    "    #logger.info(results)\n",
    "    \n",
    "    return download_orbit_dict\n",
    "\n",
    "def get_orbit_files(localize_slcs):\n",
    "    from datetime import datetime, timedelta\n",
    "    import json\n",
    "    import os\n",
    "    import osaka\n",
    "    import osaka.main\n",
    "    \n",
    "    orbit_dict = {}\n",
    "\n",
    "    orbit_dates = []\n",
    "    \n",
    "    for slc in localize_slcs:\n",
    "        match = SLC_RE.search(slc)\n",
    "        if not match:\n",
    "            raise RuntimeError(\"Failed to recognize SLC ID %s.\" %slc)\n",
    "        mission = match.group('mission')\n",
    "        day_dt_str = \"{}-{}-{}\".format(match.group('start_year'), \n",
    "                                       match.group('start_month'), match.group('start_day'))\n",
    "        \n",
    "        day_dt = datetime.strptime(day_dt_str, '%Y-%m-%d') - timedelta(days=1)\n",
    "        \n",
    "        day_dt_str = day_dt.strftime('%Y-%m-%d')\n",
    "        \n",
    "        logger.info(\"day_dt_str : {}\".format(day_dt_str))\n",
    "        if day_dt_str not in orbit_dates:\n",
    "            orbit_dates.append(day_dt_str)\n",
    "            orbit_dict = get_download_orbit_dict(orbit_dict, day_dt_str, mission)\n",
    "            directory = orbit_dir\n",
    "            if not os.path.exists(directory):\n",
    "                os.makedirs(directory)\n",
    "            #directory = os.path.join(wd, \"orbits\")\n",
    "            for k, v in orbit_dict.items():\n",
    "                osaka.main.get(v, directory)\n",
    "         \n",
    "    logger.info(\"orbit_dict : %s \" %json.dumps(orbit_dict, indent=4))\n",
    "\n",
    "def get_area(coords):\n",
    "    '''get area of enclosed coordinates- determines clockwise or counterclockwise order'''\n",
    "    from past.utils import old_div\n",
    "    \n",
    "    n = len(coords) # of corners\n",
    "    area = 0.0\n",
    "    for i in range(n):\n",
    "        j = (i + 1) % n\n",
    "        area += coords[i][1] * coords[j][0]\n",
    "        area -= coords[j][1] * coords[i][0]\n",
    "    #area = abs(area) / 2.0\n",
    "    return old_div(area, 2)\n",
    "\n",
    "def download_slc(slc_id, path):\n",
    "    url = \"https://datapool.asf.alaska.edu/SLC/SA/{}.zip\".format(slc_id)\n",
    "    logger.info(\"Downloading {} : {}\".format(slc_id, url))\n",
    "    \n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    osaka.main.get(url, path)\n",
    "\n",
    "def run_cmd_output(cmd):\n",
    "    from subprocess import check_output, CalledProcessError\n",
    "    cmd_line = \" \".join(cmd)\n",
    "    logger.info(\"Calling: {}\".format(cmd_line))\n",
    "    output = check_output(cmd_line, shell=True)\n",
    "    return output\n",
    "\n",
    "def run_cmd(cmd):\n",
    "    !source /opt/isce2/isce_env.sh\n",
    "    !export PATH=/opt/conda/bin/:/opt/isce2/src/isce2/contrib/stack/topsStack/:${PATH}\n",
    "    !export PYTHONPATH=/opt/isce2/src/isce2/contrib/stack/topsStack/:${PYTHONPATH}\n",
    "\n",
    "    import subprocess\n",
    "    from subprocess import check_call, CalledProcessError\n",
    "    import sys\n",
    "    cmd_line = \" \".join(cmd)\n",
    "    logger.info(\"Calling : {}\".format(cmd_line))\n",
    "    p = subprocess.Popen(cmd_line, shell=True,stdout=subprocess.PIPE,stderr=subprocess.STDOUT)\n",
    "    while True: \n",
    "        line = p.stdout.readline()\n",
    "        if not line:\n",
    "            break\n",
    "        logger.info(line.strip())\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "def download_dem(min_lat, max_lat, min_lon, max_lon):\n",
    "    from math import floor, ceil\n",
    "    min_lat_lo = floor(min_lat)\n",
    "    max_lat_hi = ceil(max_lat)\n",
    "    min_lon_lo = floor(min_lon)\n",
    "    max_lon_hi = ceil(max_lon)\n",
    "    \n",
    "    dem_cmd = [\n",
    "        \"{}/applications/dem.py\".format(ISCE_HOME), \"-a\",\n",
    "        \"stitch\", \"-b\", \"{} {} {} {}\".format(min_lat_lo, max_lat_hi, min_lon_lo, max_lon_hi),\n",
    "        \"-r\", \"-s\", \"1\", \"-f\", \"-c\", \"|\", \"tee\", \"dem.txt\"\n",
    "        #\"-n\", dem_user, \"-w\", dem_pass,\"-u\", dem_url\n",
    "    ]\n",
    "    run_cmd(dem_cmd)\n",
    "        \n",
    "def download_slcs(localize_slcs, path):\n",
    "    for slc in localize_slcs:\n",
    "        download_slc(slc, path) \n",
    "        \n",
    "def get_start_end_times(localize_slcs):\n",
    "    #\"S1A_IW_SLC__1SDV_20200511T135117_20200511T135144_032518_03C421_7768\"\n",
    "    import re\n",
    "    import datetime\n",
    "    \n",
    "    start_times = []\n",
    "    end_times = []\n",
    "    SLC_RE1 = re.compile(r'(?P<mission>S1\\w)_IW_SLC__.*?_(?P<start_time>\\d{8}T\\d{6})_(?P<end_time>\\d{8}T\\d{6})_.*$')\n",
    "    for slc in localize_slcs:\n",
    "        match = SLC_RE1.search(slc)\n",
    "        if not match:\n",
    "            raise RuntimeError(\"Failed to recognize SLC ID %s.\" %slc)\n",
    "        start_time_str = \"{}\".format(match.group('start_time'))\n",
    "        start_times.append(datetime.datetime.strptime(start_time_str, '%Y%m%dT%H%M%S'))\n",
    "        end_time_str = \"{}\".format(match.group('end_time'))\n",
    "        end_times.append(datetime.datetime.strptime(end_time_str, '%Y%m%dT%H%M%S'))\n",
    "    return sorted(start_times)[0], sorted(end_times)[-1]\n",
    "                                     \n",
    "        \n",
    "def xml2string(xmlroot, encoding=\"UTF-8\", method=\"xml\", indent=\"\\t\", newl=\"\\n\"):\n",
    "    from xml.dom import minidom\n",
    "    import xml.etree.cElementTree as ET\n",
    "    \n",
    "    xmlstring = minidom.parseString(ET.tostring(xmlroot, encoding=encoding, method=method))\\\n",
    "        .toprettyxml(newl=newl, indent=indent)\n",
    "    return xmlstring\n",
    "\n",
    "def create_xml(xml_file, doc_type, slcs):\n",
    "    from xml.dom import minidom \n",
    "    import xml.etree.cElementTree as ET\n",
    "    import os \n",
    "   \n",
    "    slc_list = []\n",
    "    for slc in slcs:\n",
    "        slc_list.append(os.path.join('../data/slcs/', \"{}.zip\".format(slc)))\n",
    "\n",
    "    \n",
    "\n",
    "    comp_elem = ET.Element('component')\n",
    "    comp_elem.set(\"name\", doc_type)\n",
    "\n",
    "    prop_elem = ET.SubElement(comp_elem, 'property') \n",
    "    prop_elem.set('name', 'orbit directory')\n",
    "    prop_elem.text='../data/orbits'\n",
    "\n",
    "    prop_elem = ET.SubElement(comp_elem, 'property') \n",
    "    prop_elem.set('name', 'output directory')\n",
    "    prop_elem.text=doc_type\n",
    "    \n",
    "    prop_elem = ET.SubElement(comp_elem, 'property') \n",
    "    prop_elem.set('name', 'safe')\n",
    "    prop_elem.text=str(slc_list)\n",
    "\n",
    "\n",
    "    xml_str = xml2string(comp_elem) \n",
    "\n",
    "    with open(xml_file, 'w') as fw:\n",
    "        fw.write(xml_str)\n",
    "        \n",
    "def create_dataset_json(id, version, met_file, ds_file):\n",
    "    \"\"\"Write dataset json.\"\"\"\n",
    "\n",
    "\n",
    "    # get metadata\n",
    "    with open(met_file) as f:\n",
    "        md = json.load(f)\n",
    "\n",
    "    # build dataset\n",
    "    ds = {\n",
    "        'creation_timestamp': \"%sZ\" % datetime.utcnow().isoformat(),\n",
    "        'version': version,\n",
    "        'label': id\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        \n",
    "        \n",
    "        \n",
    "        logger.info(\"create_dataset_json : met['bbox']: %s\" %md['bbox'])\n",
    "        \n",
    "        coordinates = [\n",
    "                    [\n",
    "                      [ md['bbox'][0][1], md['bbox'][0][0] ],\n",
    "                      [ md['bbox'][3][1], md['bbox'][3][0] ],\n",
    "                      [ md['bbox'][2][1], md['bbox'][2][0] ],\n",
    "                      [ md['bbox'][1][1], md['bbox'][1][0] ],\n",
    "                      [ md['bbox'][0][1], md['bbox'][0][0] ]\n",
    "                    ] \n",
    "                  ]\n",
    "        \n",
    "     \n",
    "        #coordinates = md['union_geojson']['coordinates']\n",
    "    \n",
    "        cord_area = get_area(coordinates[0])\n",
    "        if not cord_area>0:\n",
    "            logger.info(\"creating dataset json. coordinates are not clockwise, reversing it\")\n",
    "            coordinates = [coordinates[0][::-1]]\n",
    "            logger.info(coordinates)\n",
    "            cord_area = get_area(coordinates[0])\n",
    "            if not cord_area>0:\n",
    "                logger.info(\"creating dataset json. coordinates are STILL NOT  clockwise\")\n",
    "        else:\n",
    "            logger.info(\"creating dataset json. coordinates are already clockwise\")\n",
    "\n",
    "        ds['location'] =  {'type': 'Polygon', 'coordinates': coordinates}\n",
    "        logger.info(\"create_dataset_json location : %s\" %ds['location'])\n",
    "\n",
    "    except Exception as err:\n",
    "        logger.info(\"create_dataset_json: Exception : \")\n",
    "        logger.info(str(err))\n",
    "        logger.info(\"Traceback: {}\".format(traceback.format_exc()))\n",
    "\n",
    "\n",
    "    # set earliest sensing start to starttime and latest sensing stop to endtime\n",
    "    if isinstance(md['sensing_start'], str):\n",
    "        ds['starttime'] = md['sensing_start']\n",
    "    else:\n",
    "        md['sensing_start'].sort()\n",
    "        ds['starttime'] = md['sensing_start'][0]\n",
    "\n",
    "    if isinstance(md['sensing_stop'], str):\n",
    "        ds['endtime'] = md['sensing_stop']\n",
    "    else:\n",
    "        md['sensing_stop'].sort()\n",
    "        ds['endtime'] = md['sensing_stop'][-1]\n",
    "\n",
    "    # write out dataset json\n",
    "    with open(ds_file, 'w') as f:\n",
    "        json.dump(ds, f, indent=2)\n",
    "        \n",
    "\n",
    "def get_tops_subswath_xml(masterdir):\n",
    "    ''' \n",
    "        Find all available IW[1-3].xml files\n",
    "    '''\n",
    "\n",
    "    logger.info(\"get_tops_subswath_xml from : %s\" %masterdir)\n",
    "\n",
    "    masterdir = os.path.abspath(masterdir)\n",
    "    IWs = glob(os.path.join(masterdir,'IW*.xml'))\n",
    "    if len(IWs)<1:\n",
    "        raise Exception(\"Could not find a IW*.xml file in \" + masterdir)\n",
    "\n",
    "    return IWs\n",
    "\n",
    "def read_isce_product(xmlfile):\n",
    "    logger.info(\"read_isce_product: %s\" %xmlfile)\n",
    "\n",
    "    # check if the file does exist\n",
    "    check_file_exist(xmlfile)\n",
    "\n",
    "    # loading the xml file with isce\n",
    "    pm = PM()\n",
    "    pm.configure()\n",
    "    obj = pm.loadProduct(xmlfile)\n",
    "    return obj\n",
    "\n",
    "def check_file_exist(infile):\n",
    "    logger.info(\"check_file_exist : %s\" %infile)\n",
    "    if not os.path.isfile(infile):\n",
    "        raise Exception(infile + \" does not exist\")\n",
    "    else:\n",
    "        logger.info(\"%s Exists\" %infile)\n",
    "\n",
    "\n",
    "def get_tops_metadata(masterdir):\n",
    "\n",
    "    logger.info(\"get_tops_metadata from : %s\" %masterdir)\n",
    "    # get a list of avialble xml files for IW*.xml\n",
    "    IWs = get_tops_subswath_xml(masterdir)\n",
    "    # append all swaths togheter\n",
    "    frames=[]\n",
    "    for IW  in IWs:\n",
    "        logger.info(\"get_tops_metadata processing : %s\" %IW)\n",
    "        obj = read_isce_product(IW)\n",
    "        frames.append(obj)\n",
    "\n",
    "    output={}\n",
    "    dt = min(frame.sensingStart for frame in frames)\n",
    "    output['sensingStart'] =  dt.isoformat('T') + 'Z'\n",
    "    logger.info(dt)\n",
    "    dt = max(frame.sensingStop for frame in frames)\n",
    "    output['sensingStop'] = dt.isoformat('T') + 'Z'\n",
    "    logger.info(dt)\n",
    "    return output\n",
    "        \n",
    "def extract_slc_data(slc_dir, slcs):\n",
    "    from zipfile import ZipFile\n",
    "    os.chdir(slc_dir)\n",
    "    for slc in slcs:\n",
    "        i = \"{}.zip\".format(slc)\n",
    "        with ZipFile(i, 'r') as zf:\n",
    "            zf.extractall()\n",
    "            \n",
    "def create_product(insar_dir, tops_properties):\n",
    "            \n",
    "    from datetime import datetime\n",
    "    from glob import glob\n",
    "    import shutil\n",
    "\n",
    "    os.chdir(insar_dir)\n",
    "    print(insar_dir)\n",
    "\n",
    "    '''\n",
    "    output = get_tops_metadata('fine_interferogram')\n",
    "    sensing_start= output['sensingStart']\n",
    "    sensing_stop = output['sensingStop']\n",
    "    logger.info(\"sensing_start : %s\" %sensing_start)\n",
    "    logger.info(\"sensing_stop : %s\" %sensing_stop)\n",
    "    '''\n",
    "\n",
    "\n",
    "    now=datetime.utcnow().strftime(\"%Y%m%dT%H%M%S\")\n",
    "    dataset_name = \"hello_world-product-{}-{}\".format(now, \"TopsApp\")\n",
    "    start_time, end_time = get_start_end_times(localize_slcs)\n",
    "    print(\"{} : {}\".format(start_time, end_time))\n",
    "    #print(dataset_name)\n",
    "\n",
    "    prod_dir = os.path.join(insar_dir, dataset_name)\n",
    "    os.mkdir(prod_dir)\n",
    "\n",
    "    met_file = os.path.join(dataset_name, \"{}.met.json\".format(dataset_name))\n",
    "    dataset_file = os.path.join(dataset_name, \"{}.dataset.json\".format(dataset_name))\n",
    "\n",
    "    met = tops_properties\n",
    "    met[\"reference_slc\"]=reference_slcs\n",
    "    met[\"secondary_slcs\"] = secondary_slcs\n",
    "    met['sensing_start'] = start_time.isoformat('T') + 'Z'\n",
    "    met['sensing_stop'] = end_time.isoformat('T') + 'Z'\n",
    "    met['start_time'] = start_time.isoformat('T') + 'Z'\n",
    "    met['end_time'] = end_time.isoformat('T') + 'Z'\n",
    "\n",
    "    bbox = [[min_lat, min_lon], [min_lat, max_lon], [max_lat, max_lon], [max_lat, min_lon]]\n",
    "    met['bbox'] =  bbox \n",
    "    version = \"v1.0\"\n",
    "\n",
    "    # generate dataset JSON\n",
    "    with open(met_file, 'w') as f: json.dump(met, f, indent=2)\n",
    "    \n",
    "    # generate dataset JSON\n",
    "    ds_file = os.path.join(prod_dir, \"{}.dataset.json\".format(dataset_name))\n",
    "    create_dataset_json(dataset_name, version, met_file, ds_file)\n",
    "\n",
    "    merged_dir = os.path.join(insar_dir, \"merged\")\n",
    "    for name in glob(\"{}/*\".format(merged_dir)):\n",
    "    \n",
    "        input_path = os.path.join(merged_dir, name)\n",
    "        print(input_path)\n",
    "        if os.path.isfile(input_path):\n",
    "            #print(\"Copying {} to {}\".format(input_path,  prod_dir ))\n",
    "            shutil.copy(input_path,  prod_dir)\n",
    "    return prod_dir\n",
    "            \n",
    "def create_topsApp_xml():\n",
    "    from xml.dom import minidom \n",
    "    import xml.etree.cElementTree as ET\n",
    "    import os \n",
    "\n",
    "    supported_docs = os.path.join(tutorial_home_dir, 'support_docs', 'insar')\n",
    "    os.makedirs(supported_docs, exist_ok=True)\n",
    "\n",
    "\n",
    "    tops_xml_file = os.path.join(tutorial_home_dir, \"support_docs/insar/topsApp.xml\")\n",
    "\n",
    "    root = ET.Element(\"topsApp\")\n",
    "\n",
    "    comp_elem = ET.SubElement(root, 'component')\n",
    "    comp_elem.set(\"name\", 'topsinsar')\n",
    "\n",
    "    prop_elem = ET.SubElement(comp_elem, 'property') \n",
    "    prop_elem.set('name', 'Sensor name')\n",
    "    prop_elem.text=sensor_name\n",
    "\n",
    "    ref_elem = ET.SubElement(comp_elem, 'component')\n",
    "    ref_elem.set(\"name\", 'reference')\n",
    "\n",
    "    ref_cat_elem = ET.SubElement(ref_elem, 'catalog')\n",
    "    ref_cat_elem.text = \"reference.xml\"\n",
    "\n",
    "    sec_elem = ET.SubElement(comp_elem, 'component')\n",
    "    sec_elem.set(\"name\", 'secondary')\n",
    "\n",
    "    ref_cat_elem = ET.SubElement(sec_elem, 'catalog')\n",
    "    ref_cat_elem.text = \"secondary.xml\"\n",
    "\n",
    "    for k, v in tops_properties.items():\n",
    "        prop_elem = ET.SubElement(comp_elem, 'property')\n",
    "        prop_elem.set('name', k)\n",
    "        prop_elem.text = str(v)\n",
    "\n",
    "    prop_elem = ET.SubElement(comp_elem, 'property') \n",
    "    prop_elem.set('name', 'demfilename')\n",
    "    prop_elem.text=wgs84_file\n",
    "   \n",
    "    xml_str = xml2string(root) \n",
    "\n",
    "    with open(tops_xml_file, 'w') as fw:\n",
    "        #fw.write(r'<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n')\n",
    "        fw.write(xml_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## 1. Collecting Input Datasets\n",
    " - Downnloading **SLCs**\n",
    " - Downloading **Orbits**\n",
    " - Downloading **Dems**\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### 1.1 SLC Download\n",
    "\n",
    "TOPS SLC product files delivered from ESA are zip archives. When unpacked the zip extension will be replaced by SAFE. The products are therefore also frequently called SAFE files. topsApp.py can read the data from either a zip file or a SAFE file. To limit disk usage, it is recommended to not unzip the individual files.\n",
    "\n",
    "The zip or SAFE filenames provide information on the product type, the polarization, and the start and stop acquisition time. For example: S1A_IW_SLC__1SDV_20200511T135117_20200511T135144_032518_03C421_7768.zip\n",
    "- Type = slc\n",
    "- Polarization = Dual polarization\n",
    "- Date = 20200511\n",
    "- UTC time of acquisition = ~13:51\n",
    "- Sensing start for the acquisition was 20200511 at 13:51:17\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_slcs(localize_slcs, slc_dir)\n",
    "\n",
    "! ls -lh {slc_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### 1.2 Orbits Download\n",
    "\n",
    "In addition to the **SAFE files**, **orbit files** and the **auxiliary instrument files** are required for ISCE processing. Both the orbit and instrument files are provided by ESA and can be downloaded at: https://qc.sentinel1.eo.esa.int/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shutil import move\n",
    "\n",
    "os.chdir(tutorial_home_dir)\n",
    "print(tutorial_home_dir)\n",
    "\n",
    "get_orbit_files(localize_slcs)\n",
    "# Move the orbits to orbit folder\n",
    "orb_files = glob.glob(\"*.EOF\")\n",
    "for orb in orb_files:\n",
    "    move(orb, os.path.join(orbit_dir, orb))\n",
    "!ls {orbit_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Dems Download\n",
    "\n",
    "Dems over the region of intetrest is downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(insar_dir)\n",
    "download_dem(min_lat, max_lat, min_lon, max_lon)\n",
    "\n",
    "wgs84_file =''\n",
    "if os.path.exists(\"dem.txt\"):\n",
    "    cmd = [\"awk\", \"'/wgs84/ {print $NF;exit}'\", \"dem.txt\"]\n",
    "    WGS84 = run_cmd_output(cmd).decode(\"utf-8\").strip()\n",
    "    wgs84_file = os.path.join(\".\", WGS84)\n",
    "print(wgs84_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 AUX_CAL file download ####\n",
    "\n",
    "The following calibration auxliary (AUX_CAL) file is used for **antenna pattern correction** to compensate the range phase offset of SAFE products with **IPF verison 002.36** (mainly for images acquired before March 2015). If all your SAFE products are from another IPF version, then no AUX files are needed. Check [ESA document](https://earth.esa.int/documents/247904/1653440/Sentinel-1-IPF_EAP_Phase_correction) for details. \n",
    "\n",
    "Run the command below to download the AUX_CAL file once and store it somewhere (_i.e._ ~/aux/aux_cal) so that you can use it all the time, for `stackSentinel.py -a` or `auxiliary data directory` in `topsApp.py`.\n",
    "\n",
    "```\n",
    "wget https://qc.sentinel1.eo.esa.int/product/S1A/AUX_CAL/20140908T000000/S1A_AUX_CAL_V20140908T000000_G20190626T100201.SAFE.TGZ\n",
    "tar zxvf S1A_AUX_CAL_V20140908T000000_G20190626T100201.SAFE.TGZ\n",
    "rm S1A_AUX_CAL_V20140908T000000_G20190626T100201.SAFE.TGZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd $insar_dir\n",
    "mkdir -p ./AuxDir\n",
    "wget https://qc.sentinel1.eo.esa.int/product/S1A/AUX_CAL/20140908T000000/S1A_AUX_CAL_V20140908T000000_G20190626T100201.SAFE.TGZ\n",
    "tar zxvf S1A_AUX_CAL_V20140908T000000_G20190626T100201.SAFE.TGZ --directory ./AuxDir\n",
    "rm S1A_AUX_CAL_V20140908T000000_G20190626T100201.SAFE.TGZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## 2 topsApp Connfiguration Files\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "For topsApp processing, we use *topsApp.xml*, *reference.xml* and *secondary.xml*  as config files to send input data information to isce."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### 2.1 topsApp.xml\n",
    "\n",
    "Example:\n",
    "```xml\n",
    "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
    "<topsApp>\n",
    "  <component name=\"topsinsar\">\n",
    "    <property name=\"Sensor name\">SENTINEL1</property>\n",
    "    <component name=\"reference\">\n",
    "        <catalog>reference.xml</catalog>\n",
    "    </component>\n",
    "    <component name=\"secondary\">\n",
    "        <catalog>secondary.xml</catalog>\n",
    "    </component>\n",
    "    <property name=\"swaths\">[3]</property>\n",
    "    <property name=\"range looks\">7</property>\n",
    "    <property name=\"azimuth looks\">3</property>\n",
    "    <property name=\"region of interest\">[37.98, 38.33, -118.21, -117.68]</property>\n",
    "    <property name=\"do unwrap\">True</property>\n",
    "    <property name=\"unwrapper name\">snaphu_mcf</property>\n",
    "    <property name=\"do denseoffsets\">True</property>\n",
    "    <property name=\"demfilename\">path_to_your_dem</property>\n",
    "    <property name=\"geocode demfilename\">path_to_your_dem</property>\n",
    "    <!--property name=\"geocode list\">['merged/phsig.cor', 'merged/filt_topophase.unw', 'merged/los.rdr', 'merged/topophase.flat', 'merged/filt_topophase.flat','merged/topophase.cor','merged/filt_topophase.unw.conncomp']</property>-->\n",
    "  </component>\n",
    "</topsApp>\n",
    "```\n",
    "\n",
    "- The reference and secondary components refer  to their own *.xml* files \n",
    "- The **swaths** property controls the number of swaths to be processed. \n",
    "- **range looks** and **azimuth looks**: The range resolution for sentinel varies from near to far range, but is roughly 5m, while the azimuth resolution is approximately 15m, leading to a multi-looked product that will be approximately 35m by 45m.\n",
    "- By specifying the **region of interest** as [S, N, W, E] to only capture the extent, topsApp.py will only extract those bursts from subswaths needed to cover the earthquake.\n",
    "- By default, topsApp can download a DEM on the fly. By including **demFilename** a local DEM can be specified as input for the processing.\n",
    "- By default, the geocoding in topsApp.py is performed at the same sampling as processing DEM. However, a different DEM *to be used specifically for geocoding* can be specified using the **geocode demfilename** property. This is used for the case when data has been multilooked to order of 100m or greater and when geocoding to 30m is an overkill.\n",
    "- By default, no unwrapping is done. In order to turn it on, set the property **do unwrap** to *True*.\n",
    "- In case unwrapping is requested, the default unwrapping strategy to be applied is the *icu* unwrapping method. For this tutorial, we will use *snaphu_mcf*.\n",
    "- Lastly, we request topsApp.py to run the dense-offsets using the **do denseoffsets** property. By enabling this, topsApp.py will estimate the range and azimuth offsets on the amplitude of the reference and secondary SLC files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_topsApp_xml()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### 2.2 reference.xml and secondary.xml\n",
    "\n",
    "Example:\n",
    "``` xml\n",
    "<component name=\"reference\">\n",
    "    <property name=\"orbit directory\">../data/orbits</property>\n",
    "    <property name=\"output directory\">reference</property>\n",
    "    <property name=\"safe\">['../data/slcs/S1A_IW_SLC__1SDV_20200511T135117_20200511T135144_032518_03C421_7768.zip']</property>\n",
    "</component>\n",
    "```\n",
    "- The value associated with the reference **safe** property corresponds to a list of SAFE files that are to be mosaiced when generating the interferogram. \n",
    "- The **orbit directory** points  to the directory where we have stored the POEORB (precise) orbits for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xml_file = os.path.join(tutorial_home_dir, \"support_docs/insar/reference.xml\")\n",
    "create_xml(xml_file, 'reference', reference_slcs)\n",
    "\n",
    "xml_file = os.path.join(tutorial_home_dir, \"support_docs/insar/secondary.xml\")\n",
    "create_xml(xml_file, 'secondary', secondary_slcs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Moving input XML Files in Appropriate Directory\n",
    "\n",
    "Moving the files in insar directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Template xml for this tutorial\n",
    "topsAppXml_original =  os.path.join(tutorial_home_dir,'support_docs/insar/topsApp.xml')  \n",
    "refXml_original =  os.path.join(tutorial_home_dir,'support_docs/insar/reference.xml')  \n",
    "secXml_original =  os.path.join(tutorial_home_dir,'support_docs/insar/secondary.xml')  \n",
    "\n",
    "\n",
    "## Check if the topsApp.xml file already exists, if not copy the example for the excerisize\n",
    "if not os.path.isfile(os.path.join(insar_dir,'topsApp.xml')):\n",
    "    copyfile(topsAppXml_original, os.path.join(insar_dir, 'topsApp.xml'))\n",
    "else:\n",
    "    print(os.path.join(insar_dir,'topsApp.xml') + \" already exist, will not overwrite\")\n",
    "    \n",
    "if not os.path.isfile(os.path.join(insar_dir, 'reference.xml')):\n",
    "    copyfile(refXml_original, os.path.join(insar_dir,'reference.xml'))\n",
    "else:\n",
    "    print(os.path.join(insar_dir,'reference.xml') + \" already exist, will not overwrite\")\n",
    "    \n",
    "if not os.path.isfile(os.path.join(insar_dir, 'secondary.xml')):\n",
    "    copyfile(secXml_original,os.path.join(insar_dir, 'secondary.xml'))\n",
    "else:\n",
    "    print(os.path.join(insar_dir,'secondary.xml') + \" already exist, will not overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## 3. topsApp.py processing steps\n",
    "\n",
    "The topsApp.py workflow can be called with a single command-line call to topsApp.py; by default it will run all the required processing steps with inputs pulled from the topsApp.xml file. Although this is an attractive feature, it is recommended to run topsApp.py with “steps” enabled. This will allow you to re-start the processing from a given processing step. If “steps” are not used, users must restart processing from the beginning of the workflow after fixing any downstream issues with the processing.\n",
    "\n",
    "\n",
    "**Steps of topsApp processing (in order)**:\n",
    " - startup\n",
    " - preproces\n",
    " - computeBaselines\n",
    " - verifyDEM\n",
    " - topo\n",
    " - subsetoverlaps\n",
    " - coarseoffsets\n",
    " - coarseresamp\n",
    " - overlapifg\n",
    " - prepesd\n",
    " - esd\n",
    " - rangecoreg\n",
    " - fineoffsets\n",
    " - fineresamp\n",
    " - ion\n",
    " - burstifg\n",
    " - mergebursts\n",
    " - filter\n",
    " - unwrap\n",
    " - unwrap2stage\n",
    " - geocode\n",
    " - denseoffsets\n",
    " - filteroffsets\n",
    " - geocodeoffsets\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "!/opt/isce2/isce/applications/topsApp.py --help --steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### 3.1 START THE PROCESSING\n",
    "We will do processing from \"startup\" to \"geocode\" in one step in working directory (insar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "os.chdir(insar_dir)\n",
    "!/opt/isce2/isce/applications/topsApp.py --start=startup --end=geocode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 PLOT  INTERFEROGRAM\n",
    "\n",
    "Plotting filt_topophase.unw.geo with metaplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(insar_dir)\n",
    "\n",
    "file = os.path.join(insar_dir, 'merged', 'filt_topophase.flat.geo')\n",
    "print(file)\n",
    "\n",
    "'''\n",
    "plotdata('merged/filt_topophase.unw.geo', band=2,\n",
    "         title=\"UNW GEO FILT IFG [rad] \", colormap='jet',\n",
    "         colorbar_orientation=\"vertical\", datamin=-20, datamax=50)\n",
    "'''\n",
    "try:\n",
    "    plot_wrapped_data_multiframe([file])\n",
    "except Exception as e:\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Generate the Product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prod_name = create_product(insar_dir, tops_properties)\n",
    "\n",
    "print(prod_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relevant references:\n",
    "- Heresh Fattahi, Piyush Agram, and Mark Simons, *Precise coregistration of Sentinel-1A TOPS data*, https://files.scec.org/s3fs-public/0129_1400_1530_Fattahi.pdf\n",
    "- Fattahi, H., Agram, P. and Simons, M., 2016. A network-based enhanced spectral diversity approach for TOPS time-series analysis. IEEE Transactions on Geoscience and Remote Sensing, 55(2), pp.777-786. https://core.ac.uk/reader/77927508\n",
    "- ESA, *Sentinel-1 and TOPS overview*, https://sentinel.esa.int/web/sentinel/user-guides/sentinel-1-sar\n",
    "- Nestor Yague-Martinez, Pau Prats-Iraola, Fernando Rodriguez Gonzalez,Ramon Brcic, Robert Shau, Dirk Geudtner, Michael Eineder, and Richard Bamler, *Interferometric Processing of Sentinel-1 TOPS Data*, IEEE, doi:10.1109/TGRS.2015.2497902, https://ieeexplore.ieee.org/document/7390052/\n",
    "- Liang, C., Agram, P., Simons, M. and Fielding, E.J., 2019. Ionospheric correction of insar time series analysis of c-band sentinel-1 tops data. IEEE Transactions on Geoscience and Remote Sensing, 57(9), pp.6755-6773. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"1\">This notebook is compatible with NISAR Jupyter Server Stack v1.4 and above</font>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "288px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
